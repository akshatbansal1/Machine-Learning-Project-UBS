{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pycountry\n",
    "import pycountry_convert as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = 'data/raw/'\n",
    "output_path = 'data/preprocessed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data from disk (ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = pd.read_csv(raw_data_path+'degrees.csv')\n",
    "event_appearances = pd.read_csv(raw_data_path+'event_appearances.csv')\n",
    "jobs = pd.read_csv(raw_data_path+'jobs.csv')\n",
    "org_parents = pd.read_csv(raw_data_path+'org_parents.csv')\n",
    "organizations = pd.read_csv(raw_data_path+'organizations.csv')\n",
    "people = pd.read_csv(raw_data_path+'people.csv')\n",
    "\n",
    "acquisitions = pd.read_csv(raw_data_path+'acquisitions.csv')\n",
    "funding_rounds = pd.read_csv(raw_data_path+'funding_rounds.csv')\n",
    "ipos = pd.read_csv(raw_data_path+'ipos.csv')\n",
    "investors = pd.read_csv(raw_data_path+'investors.csv')\n",
    "investments = pd.read_csv(raw_data_path+'investments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def convert_datetime(df):\n",
    "    cols = ['created_at', 'updated_at', 'founded_on', 'last_funding_on', 'closed_on','started_on','completed_on','ended_on','PreSeries_announced_on','RoundA_announced_on','RoundB_announced_on','RoundC_announced_on','RoundD_announced_on','went_public_on']\n",
    "    cols2 = ['age_closed','age_operating','PreSeries_from_founded','RoundA_from_founded', 'RoundA_from_PreSeries', 'RoundB_from_founded','RoundB_from_PreSeries','RoundB_from_RoundA','RoundC_from_founded','RoundC_from_PreSeries','RoundC_from_RoundA','RoundC_from_RoundB','RoundD_from_founded','RoundD_from_PreSeries','RoundD_from_RoundA','RoundD_from_RoundB','RoundD_from_RoundC']    \n",
    "    for col, data in df.iteritems():\n",
    "        df[col] = pd.to_datetime(df[col], errors = 'coerce') if col in cols else df[col] # for out-of-bound datetimes -> NaT\n",
    "    for col, data in df.iteritems():\n",
    "        df[col] = pd.to_timedelta(df[col], errors = 'coerce') if col in cols2 else df[col] # for out-of-bound datetimes -> NaT\n",
    "    return df\n",
    "\n",
    "# for part 1------------------------\n",
    "# for getting type of organization -> company, investor, or school\n",
    "def feature_to_list(x):\n",
    "    x = x.strip().split(',')\n",
    "    if 'company' not in x:\n",
    "        x.append('c')\n",
    "    if 'investor' not in x:\n",
    "        x.append('i')\n",
    "    if 'school' not in x:\n",
    "        x.append('s')\n",
    "    x.sort()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 'c' or x[i] == 'i' or x[i] == 's':\n",
    "            x[i] = None\n",
    "    return x\n",
    "\n",
    "today = date.today()\n",
    "def get_age_operating(d):\n",
    "    try:\n",
    "        res = pd.to_timedelta(today - d.to_pydatetime().date())\n",
    "    except:\n",
    "        res = pd.Timedelta('292y')\n",
    "    return res\n",
    "\n",
    "# for part 4-----------------------\n",
    "# split roles into separate columns\n",
    "def split_categories(x):\n",
    "    return x.strip().split(',')\n",
    "\n",
    "def concat_to_str(x):\n",
    "    str_ = ''\n",
    "    for i in x:\n",
    "        str_ += i.strip()\n",
    "        str_ += ','\n",
    "    return str_[:-1] if len(str_) else ''\n",
    "\n",
    "def strip_uninterested_categories(x, categories_of_interest):\n",
    "    if x is np.nan:\n",
    "        return x\n",
    "    category_list = []\n",
    "    x = split_categories(x)\n",
    "    for category in x:\n",
    "        if category in categories_of_interest:\n",
    "            category_list.append(category)\n",
    "    return concat_to_str(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations = convert_datetime(organizations)\n",
    "degrees = convert_datetime(degrees)\n",
    "event_appearances = convert_datetime(event_appearances)\n",
    "jobs = convert_datetime(jobs)\n",
    "org_parents = convert_datetime(org_parents)\n",
    "people = convert_datetime(people)\n",
    "acquisitions = convert_datetime(acquisitions)\n",
    "funding_rounds = convert_datetime(funding_rounds)\n",
    "ipos = convert_datetime(ipos)\n",
    "investors = convert_datetime(investors)\n",
    "investments = convert_datetime(investments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1230973 entries, 0 to 1230972\n",
      "Data columns (total 41 columns):\n",
      " #   Column                       Non-Null Count    Dtype         \n",
      "---  ------                       --------------    -----         \n",
      " 0   uuid                         1230973 non-null  object        \n",
      " 1   name                         1230959 non-null  object        \n",
      " 2   type                         1230973 non-null  object        \n",
      " 3   permalink                    1230966 non-null  object        \n",
      " 4   cb_url                       1230967 non-null  object        \n",
      " 5   rank                         1230798 non-null  float64       \n",
      " 6   created_at                   1230973 non-null  datetime64[ns]\n",
      " 7   updated_at                   1230973 non-null  datetime64[ns]\n",
      " 8   legal_name                   187626 non-null   object        \n",
      " 9   roles                        1221975 non-null  object        \n",
      " 10  domain                       1151205 non-null  object        \n",
      " 11  homepage_url                 1151206 non-null  object        \n",
      " 12  country_code                 1040790 non-null  object        \n",
      " 13  state_code                   466482 non-null   object        \n",
      " 14  region                       1040790 non-null  object        \n",
      " 15  city                         1040790 non-null  object        \n",
      " 16  address                      572025 non-null   object        \n",
      " 17  postal_code                  541661 non-null   object        \n",
      " 18  status                       1230973 non-null  object        \n",
      " 19  short_description            1230854 non-null  object        \n",
      " 20  category_list                1151905 non-null  object        \n",
      " 21  category_groups_list         1151905 non-null  object        \n",
      " 22  num_funding_rounds           193174 non-null   float64       \n",
      " 23  total_funding_usd            141546 non-null   float64       \n",
      " 24  total_funding                141546 non-null   float64       \n",
      " 25  total_funding_currency_code  141546 non-null   object        \n",
      " 26  founded_on                   967247 non-null   datetime64[ns]\n",
      " 27  last_funding_on              193125 non-null   datetime64[ns]\n",
      " 28  closed_on                    14301 non-null    datetime64[ns]\n",
      " 29  employee_count               1230973 non-null  object        \n",
      " 30  email                        764460 non-null   object        \n",
      " 31  phone                        822322 non-null   object        \n",
      " 32  facebook_url                 649299 non-null   object        \n",
      " 33  linkedin_url                 579101 non-null   object        \n",
      " 34  twitter_url                  590678 non-null   object        \n",
      " 35  logo_url                     1041253 non-null  object        \n",
      " 36  alias1                       187747 non-null   object        \n",
      " 37  alias2                       33392 non-null    object        \n",
      " 38  alias3                       14336 non-null    object        \n",
      " 39  primary_role                 1230973 non-null  object        \n",
      " 40  num_exits                    17875 non-null    float64       \n",
      "dtypes: datetime64[ns](5), float64(5), object(31)\n",
      "memory usage: 385.1+ MB\n"
     ]
    }
   ],
   "source": [
    "organizations.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369990 entries, 0 to 369989\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   uuid              369990 non-null  object        \n",
      " 1   name              339822 non-null  object        \n",
      " 2   type              369990 non-null  object        \n",
      " 3   permalink         0 non-null       float64       \n",
      " 4   cb_url            0 non-null       float64       \n",
      " 5   rank              0 non-null       float64       \n",
      " 6   created_at        369990 non-null  datetime64[ns]\n",
      " 7   updated_at        369990 non-null  datetime64[ns]\n",
      " 8   person_uuid       369990 non-null  object        \n",
      " 9   person_name       369990 non-null  object        \n",
      " 10  institution_uuid  369990 non-null  object        \n",
      " 11  institution_name  369990 non-null  object        \n",
      " 12  degree_type       358151 non-null  object        \n",
      " 13  subject           342822 non-null  object        \n",
      " 14  started_on        147915 non-null  datetime64[ns]\n",
      " 15  completed_on      200628 non-null  datetime64[ns]\n",
      " 16  is_completed      369990 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](4), float64(3), object(9)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "degrees.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 413422 entries, 0 to 413421\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   uuid               413422 non-null  object        \n",
      " 1   name               413422 non-null  object        \n",
      " 2   type               413422 non-null  object        \n",
      " 3   permalink          413422 non-null  object        \n",
      " 4   cb_url             413422 non-null  object        \n",
      " 5   rank               0 non-null       float64       \n",
      " 6   created_at         413422 non-null  datetime64[ns]\n",
      " 7   updated_at         413422 non-null  datetime64[ns]\n",
      " 8   event_uuid         413422 non-null  object        \n",
      " 9   event_name         413422 non-null  object        \n",
      " 10  participant_uuid   413422 non-null  object        \n",
      " 11  participant_name   413422 non-null  object        \n",
      " 12  participant_type   413422 non-null  object        \n",
      " 13  appearance_type    413422 non-null  object        \n",
      " 14  short_description  53824 non-null   object        \n",
      "dtypes: datetime64[ns](2), float64(1), object(12)\n",
      "memory usage: 47.3+ MB\n"
     ]
    }
   ],
   "source": [
    "event_appearances.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1589222 entries, 0 to 1589221\n",
      "Data columns (total 17 columns):\n",
      " #   Column       Non-Null Count    Dtype         \n",
      "---  ------       --------------    -----         \n",
      " 0   uuid         1589222 non-null  object        \n",
      " 1   name         1589222 non-null  object        \n",
      " 2   type         1589222 non-null  object        \n",
      " 3   permalink    1589220 non-null  object        \n",
      " 4   cb_url       1589220 non-null  object        \n",
      " 5   rank         0 non-null        float64       \n",
      " 6   created_at   1589222 non-null  datetime64[ns]\n",
      " 7   updated_at   1589222 non-null  datetime64[ns]\n",
      " 8   person_uuid  1589222 non-null  object        \n",
      " 9   person_name  1589222 non-null  object        \n",
      " 10  org_uuid     1589222 non-null  object        \n",
      " 11  org_name     1589218 non-null  object        \n",
      " 12  started_on   786363 non-null   datetime64[ns]\n",
      " 13  ended_on     293929 non-null   datetime64[ns]\n",
      " 14  is_current   1589222 non-null  bool          \n",
      " 15  title        1589141 non-null  object        \n",
      " 16  job_type     1589222 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](4), float64(1), object(11)\n",
      "memory usage: 195.5+ MB\n"
     ]
    }
   ],
   "source": [
    "jobs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16982 entries, 0 to 16981\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   uuid         16982 non-null  object        \n",
      " 1   name         16982 non-null  object        \n",
      " 2   type         16982 non-null  object        \n",
      " 3   permalink    16982 non-null  object        \n",
      " 4   cb_url       16982 non-null  object        \n",
      " 5   rank         16982 non-null  int64         \n",
      " 6   created_at   16982 non-null  datetime64[ns]\n",
      " 7   updated_at   16982 non-null  datetime64[ns]\n",
      " 8   parent_uuid  16982 non-null  object        \n",
      " 9   parent_name  16982 non-null  object        \n",
      "dtypes: datetime64[ns](2), int64(1), object(7)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "org_parents.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1106474 entries, 0 to 1106473\n",
      "Data columns (total 22 columns):\n",
      " #   Column                          Non-Null Count    Dtype         \n",
      "---  ------                          --------------    -----         \n",
      " 0   uuid                            1106474 non-null  object        \n",
      " 1   name                            1106474 non-null  object        \n",
      " 2   type                            1106474 non-null  object        \n",
      " 3   permalink                       1106473 non-null  object        \n",
      " 4   cb_url                          1106473 non-null  object        \n",
      " 5   rank                            1104160 non-null  float64       \n",
      " 6   created_at                      1106474 non-null  datetime64[ns]\n",
      " 7   updated_at                      1106474 non-null  datetime64[ns]\n",
      " 8   first_name                      1106443 non-null  object        \n",
      " 9   last_name                       1106444 non-null  object        \n",
      " 10  gender                          1084964 non-null  object        \n",
      " 11  country_code                    619658 non-null   object        \n",
      " 12  state_code                      319187 non-null   object        \n",
      " 13  region                          574716 non-null   object        \n",
      " 14  city                            558620 non-null   object        \n",
      " 15  featured_job_organization_uuid  676013 non-null   object        \n",
      " 16  featured_job_organization_name  676012 non-null   object        \n",
      " 17  featured_job_title              675999 non-null   object        \n",
      " 18  facebook_url                    95827 non-null    object        \n",
      " 19  linkedin_url                    637489 non-null   object        \n",
      " 20  twitter_url                     194938 non-null   object        \n",
      " 21  logo_url                        816186 non-null   object        \n",
      "dtypes: datetime64[ns](2), float64(1), object(19)\n",
      "memory usage: 185.7+ MB\n"
     ]
    }
   ],
   "source": [
    "people.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109825 entries, 0 to 109824\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   uuid                   109825 non-null  object        \n",
      " 1   name                   109825 non-null  object        \n",
      " 2   type                   109825 non-null  object        \n",
      " 3   permalink              109825 non-null  object        \n",
      " 4   cb_url                 109825 non-null  object        \n",
      " 5   rank                   109824 non-null  float64       \n",
      " 6   created_at             109825 non-null  datetime64[ns]\n",
      " 7   updated_at             109825 non-null  datetime64[ns]\n",
      " 8   acquiree_uuid          109825 non-null  object        \n",
      " 9   acquiree_name          109825 non-null  object        \n",
      " 10  acquiree_cb_url        109825 non-null  object        \n",
      " 11  acquiree_country_code  103573 non-null  object        \n",
      " 12  acquiree_state_code    56940 non-null   object        \n",
      " 13  acquiree_region        103573 non-null  object        \n",
      " 14  acquiree_city          103573 non-null  object        \n",
      " 15  acquirer_uuid          109825 non-null  object        \n",
      " 16  acquirer_name          109825 non-null  object        \n",
      " 17  acquirer_cb_url        109825 non-null  object        \n",
      " 18  acquirer_country_code  108811 non-null  object        \n",
      " 19  acquirer_state_code    63000 non-null   object        \n",
      " 20  acquirer_region        108811 non-null  object        \n",
      " 21  acquirer_city          108811 non-null  object        \n",
      " 22  acquisition_type       100267 non-null  object        \n",
      " 23  acquired_on            109825 non-null  object        \n",
      " 24  price_usd              19053 non-null   float64       \n",
      " 25  price                  19053 non-null   float64       \n",
      " 26  price_currency_code    19053 non-null   object        \n",
      "dtypes: datetime64[ns](2), float64(3), object(22)\n",
      "memory usage: 22.6+ MB\n"
     ]
    }
   ],
   "source": [
    "acquisitions.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 366609 entries, 0 to 366608\n",
      "Data columns (total 24 columns):\n",
      " #   Column                              Non-Null Count   Dtype         \n",
      "---  ------                              --------------   -----         \n",
      " 0   uuid                                366609 non-null  object        \n",
      " 1   name                                366609 non-null  object        \n",
      " 2   type                                366609 non-null  object        \n",
      " 3   permalink                           366608 non-null  object        \n",
      " 4   cb_url                              366608 non-null  object        \n",
      " 5   rank                                366562 non-null  float64       \n",
      " 6   created_at                          366609 non-null  datetime64[ns]\n",
      " 7   updated_at                          366609 non-null  datetime64[ns]\n",
      " 8   country_code                        362335 non-null  object        \n",
      " 9   state_code                          192266 non-null  object        \n",
      " 10  region                              362335 non-null  object        \n",
      " 11  city                                362335 non-null  object        \n",
      " 12  investment_type                     366609 non-null  object        \n",
      " 13  announced_on                        366609 non-null  object        \n",
      " 14  raised_amount_usd                   267271 non-null  float64       \n",
      " 15  raised_amount                       267271 non-null  float64       \n",
      " 16  raised_amount_currency_code         267271 non-null  object        \n",
      " 17  post_money_valuation_usd            18189 non-null   float64       \n",
      " 18  post_money_valuation                18189 non-null   float64       \n",
      " 19  post_money_valuation_currency_code  18189 non-null   object        \n",
      " 20  investor_count                      264335 non-null  float64       \n",
      " 21  org_uuid                            366609 non-null  object        \n",
      " 22  org_name                            366607 non-null  object        \n",
      " 23  lead_investor_uuids                 126999 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(6), object(16)\n",
      "memory usage: 67.1+ MB\n"
     ]
    }
   ],
   "source": [
    "funding_rounds.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35244 entries, 0 to 35243\n",
      "Data columns (total 27 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   uuid                           35244 non-null  object        \n",
      " 1   name                           0 non-null      float64       \n",
      " 2   type                           35244 non-null  object        \n",
      " 3   permalink                      35244 non-null  object        \n",
      " 4   cb_url                         35244 non-null  object        \n",
      " 5   rank                           35239 non-null  float64       \n",
      " 6   created_at                     35244 non-null  datetime64[ns]\n",
      " 7   updated_at                     35244 non-null  datetime64[ns]\n",
      " 8   org_uuid                       35244 non-null  object        \n",
      " 9   org_name                       35244 non-null  object        \n",
      " 10  org_cb_url                     35244 non-null  object        \n",
      " 11  country_code                   35119 non-null  object        \n",
      " 12  state_code                     11879 non-null  object        \n",
      " 13  region                         35119 non-null  object        \n",
      " 14  city                           35119 non-null  object        \n",
      " 15  stock_exchange_symbol          35186 non-null  object        \n",
      " 16  stock_symbol                   35225 non-null  object        \n",
      " 17  went_public_on                 35244 non-null  datetime64[ns]\n",
      " 18  share_price_usd                4432 non-null   float64       \n",
      " 19  share_price                    4432 non-null   float64       \n",
      " 20  share_price_currency_code      4432 non-null   object        \n",
      " 21  valuation_price_usd            2176 non-null   float64       \n",
      " 22  valuation_price                2176 non-null   float64       \n",
      " 23  valuation_price_currency_code  2176 non-null   object        \n",
      " 24  money_raised_usd               4954 non-null   float64       \n",
      " 25  money_raised                   4954 non-null   float64       \n",
      " 26  money_raised_currency_code     4954 non-null   object        \n",
      "dtypes: datetime64[ns](3), float64(8), object(16)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "ipos.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168191 entries, 0 to 168190\n",
      "Data columns (total 25 columns):\n",
      " #   Column                       Non-Null Count   Dtype         \n",
      "---  ------                       --------------   -----         \n",
      " 0   uuid                         168191 non-null  object        \n",
      " 1   name                         168189 non-null  object        \n",
      " 2   type                         168191 non-null  object        \n",
      " 3   permalink                    168191 non-null  object        \n",
      " 4   cb_url                       168191 non-null  object        \n",
      " 5   rank                         167816 non-null  float64       \n",
      " 6   created_at                   168191 non-null  datetime64[ns]\n",
      " 7   updated_at                   168191 non-null  datetime64[ns]\n",
      " 8   roles                        168191 non-null  object        \n",
      " 9   domain                       66476 non-null   object        \n",
      " 10  country_code                 113599 non-null  object        \n",
      " 11  state_code                   54511 non-null   object        \n",
      " 12  region                       109525 non-null  object        \n",
      " 13  city                         107870 non-null  object        \n",
      " 14  investor_types               86280 non-null   object        \n",
      " 15  investment_count             83605 non-null   float64       \n",
      " 16  total_funding_usd            4196 non-null    float64       \n",
      " 17  total_funding                4196 non-null    float64       \n",
      " 18  total_funding_currency_code  4196 non-null    object        \n",
      " 19  founded_on                   45568 non-null   datetime64[ns]\n",
      " 20  closed_on                    436 non-null     datetime64[ns]\n",
      " 21  facebook_url                 29351 non-null   object        \n",
      " 22  linkedin_url                 88855 non-null   object        \n",
      " 23  twitter_url                  42952 non-null   object        \n",
      " 24  logo_url                     125004 non-null  object        \n",
      "dtypes: datetime64[ns](4), float64(4), object(17)\n",
      "memory usage: 32.1+ MB\n"
     ]
    }
   ],
   "source": [
    "investors.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 552420 entries, 0 to 552419\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   uuid                552420 non-null  object        \n",
      " 1   name                552417 non-null  object        \n",
      " 2   type                552420 non-null  object        \n",
      " 3   permalink           552416 non-null  object        \n",
      " 4   cb_url              552416 non-null  object        \n",
      " 5   rank                0 non-null       float64       \n",
      " 6   created_at          552420 non-null  datetime64[ns]\n",
      " 7   updated_at          552420 non-null  datetime64[ns]\n",
      " 8   funding_round_uuid  552419 non-null  object        \n",
      " 9   funding_round_name  552417 non-null  object        \n",
      " 10  investor_uuid       552420 non-null  object        \n",
      " 11  investor_name       552419 non-null  object        \n",
      " 12  investor_type       552420 non-null  object        \n",
      " 13  is_lead_investor    266915 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), object(11)\n",
      "memory usage: 59.0+ MB\n"
     ]
    }
   ],
   "source": [
    "investments.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1 - Preprocessing organizational and managerial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_organizations = organizations.drop([\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "#     'rank', # need for extracting schools' rank, drop later\n",
    "    'permalink',\n",
    "    'legal_name',\n",
    "#     'cb_url', # for front-end dataset\n",
    "    'address',\n",
    "    'phone',\n",
    "#     'homepage_url', # for front-end dataset\n",
    "    'logo_url',\n",
    "    'postal_code',\n",
    "    'region',\n",
    "    'city',\n",
    "    'short_description',\n",
    "    'alias1', \n",
    "    'alias2',\n",
    "    'alias3',\n",
    "    'total_funding',# total_funding_usd is used\n",
    "    'total_funding_currency_code',\n",
    "    'email',\n",
    "    'type', # all = 'organization'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping\n",
    "organizations_drop = distil_organizations.copy()\n",
    "\n",
    "# remove rows without company name (checked, they don't have alias either)\n",
    "organizations_drop = organizations_drop[organizations_drop['name'].notnull()]\n",
    "\n",
    "# remove illogical instances with founded_on < closed_on\n",
    "organizations_drop = organizations_drop[~(organizations_drop['closed_on'] < organizations_drop['founded_on'])]\n",
    "organizations_drop = organizations_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "organizations_fill = organizations_drop.copy()\n",
    "\n",
    "# fill null in 'num_exits' as 0\n",
    "organizations_fill['num_exits'] = organizations_fill['num_exits'].fillna(0).astype(int)\n",
    "\n",
    "# if 'roles' is empty, get value from 'primary_role'\n",
    "organizations_fill['roles'] = organizations_fill['roles'].fillna(organizations_fill['primary_role'])\n",
    "\n",
    "# if missing country code or state code, fill with \"Unknown\"\n",
    "organizations_fill['country_code'] = organizations_fill['country_code'].fillna('Unknown')\n",
    "organizations_fill['state_code'] = organizations_fill['state_code'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda3\\envs\\TEMG4952A\\lib\\site-packages\\pandas\\core\\series.py:4108: FutureWarning: Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and will be removed in a future version\n",
      "  mapped = lib.map_infer(values, f, convert=convert_dtype)\n"
     ]
    }
   ],
   "source": [
    "# transforming\n",
    "organizations_transform = organizations_fill.copy()\n",
    "\n",
    "# change to 1 flag: has_domain, this signifies whether the firm is well-developed/ has influence\n",
    "organizations_transform['has_domain'] = organizations_transform['domain'].notnull()\n",
    "\n",
    "# split roles into separate columns\n",
    "organizations_transform['roles'] = organizations_transform['roles'].apply(func = feature_to_list)\n",
    "organizations_transform[['role1','role2','role3']] = pd.DataFrame(organizations_transform.roles.tolist())\n",
    "organizations_transform = organizations_transform.reset_index(drop=True)\n",
    "\n",
    "# change 'roles' to 3 flags: is_company, is_investor, is_school,is_primary_company, is_primary_investor, is_primary_school\n",
    "roles_dummy = pd.get_dummies(organizations_transform[['role1','role2','role3']], prefix = 'is')\n",
    "primary_role_dummy = pd.get_dummies(organizations_transform['primary_role'], prefix = 'is_primary')\n",
    "dummy = pd.concat((roles_dummy, primary_role_dummy), axis = 1) \n",
    "organizations_transform = pd.concat((organizations_transform, dummy), axis = 1) # add 6 columns to main org dataframe\n",
    "organizations_transform = organizations_transform.reset_index(drop=True)\n",
    "\n",
    "# change to 4 flags: is_acquired, is_ipo, is_operating, is_closed\n",
    "status_dummy = pd.get_dummies(organizations_transform['status'], prefix = 'is') # 4 one-hot encoded columns\n",
    "organizations_transform = pd.concat((organizations_transform, status_dummy), axis = 1) # add 4 columns to main org dataframe\n",
    "organizations_transform = organizations_transform.reset_index(drop=True)\n",
    "organizations_transform['is_operating'] = organizations_transform.where(organizations_transform['is_closed']==1, 1)['is_operating']\n",
    "\n",
    "# create age_closed (for company that closed), age_operating (how old the firm is today) for every company in days\n",
    "age_closed = organizations_transform[organizations_transform['closed_on'].notnull()]['closed_on'] - organizations_transform[organizations_transform['closed_on'].notnull()]['founded_on']\n",
    "age_closed = pd.DataFrame({'age_closed': age_closed})\n",
    "organizations_transform = pd.concat((organizations_transform, age_closed), axis = 1) # add age_closed to main org dataframe\n",
    "organizations_transform = organizations_transform.reset_index(drop=True)\n",
    "\n",
    "age_operating = organizations_transform[organizations_transform['closed_on'].isnull() & organizations_transform['founded_on'].notnull()]['founded_on']\n",
    "age_operating = age_operating.apply(func=get_age_operating)\n",
    "age_operating = pd.DataFrame({'age_operating': age_operating})\n",
    "organizations_transform = pd.concat((organizations_transform, age_operating), axis = 1) # add age_operating to main org dataframe\n",
    "organizations_transform = organizations_transform.reset_index(drop=True)\n",
    "\n",
    "# convert facebook/linkedin/twitter urls to number_of_social_media_url, it signifies how much social influence the firm has\n",
    "organizations_transform['has_facebook'] = organizations_transform['facebook_url'].notnull().astype(int)\n",
    "organizations_transform['has_linkedin'] = organizations_transform['linkedin_url'].notnull().astype(int)\n",
    "organizations_transform['has_twitter'] = organizations_transform['twitter_url'].notnull().astype(int)\n",
    "organizations_transform['num_social_media_url_org'] = organizations_transform['has_facebook']+organizations_transform['has_linkedin']+organizations_transform['has_twitter']\n",
    "\n",
    "# convert founded_on and closed_on from timestamp to year\n",
    "organizations_transform['founded_on_year'] = organizations_transform['founded_on'].dt.year\n",
    "organizations_transform['closed_on_year'] = organizations_transform['closed_on'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a list of top_500_schools, can be later used for identifying educational backgrounds of founders/emplyees\n",
    "# schools are ranked based on crunchbase's given rank\n",
    "top_500_schools = organizations_transform[organizations_transform['is_school']==1].sort_values(by=['rank']).head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_transform = organizations_transform.drop([\n",
    "    'roles',\n",
    "    'role1',\n",
    "    'role2',\n",
    "    'role3',\n",
    "    'primary_role',\n",
    "    'status',\n",
    "#     'domain', # for front-end usage\n",
    "#     'facebook_url', \n",
    "#     'linkedin_url',\n",
    "#     'twitter_url',\n",
    "    'rank'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking\n",
    "# organizations_transform.info()\n",
    "# organizations_transform.head(10)\n",
    "# organizations_transform.to_csv('organizations_transform.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_degrees = degrees.drop([\n",
    "    'permalink',\n",
    "    'cb_url',\n",
    "    'rank', # all null\n",
    "    'type', # all == 'degree' \n",
    "    'name', # name is basically institution name + subject\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "    'started_on', # not meaningful, and half of them are null\n",
    "    'completed_on' # not meaningful, and half of them are null\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping\n",
    "degrees_drop = distil_degrees.copy()\n",
    "\n",
    "# remove rows without subject name because they are not credible\n",
    "degrees_drop = degrees_drop[degrees_drop['subject'].notnull()] \n",
    "\n",
    "# too costly to process(deriving degree_type), remove instances without degree_type\n",
    "degrees_drop = degrees_drop[degrees_drop['degree_type'].notnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "degrees_fill = degrees_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming\n",
    "degrees_transform = degrees_fill.copy()\n",
    "\n",
    "# add is_from_top_500_schools to identify 'good' educational background. uses rank prepared from organizations\n",
    "top_500_schools = pd.DataFrame({'institution_uuid': top_500_schools['uuid'], 'is_from_top_500_schools': 1})\n",
    "degrees_transform = degrees_transform.merge(top_500_schools, how='left', on='institution_uuid') \n",
    "degrees_transform['is_from_top_500_schools'] = degrees_transform['is_from_top_500_schools'].fillna(0) # 0 means the degree is not from top schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking\n",
    "# degrees_transform.info()\n",
    "# degrees_transform.head(10)\n",
    "# degrees_transform.to_csv('degrees_transform.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. event_appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_event_appearances = event_appearances.drop([\n",
    "    'permalink', # not useful\n",
    "    'cb_url', # not useful\n",
    "    'short_description', # wedon't do sentiment analysis\n",
    "    'rank', # all null\n",
    "    'created_at', # not useful\n",
    "    'updated_at', # not useful\n",
    "    'type', # not meaningful, they are all 'event_appearance'\n",
    "    'event_name', # not useful, we only count exposure by number\n",
    "    'name' # basically event name + organizer\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking:\n",
    "# no processing needed in this stage. We only count the number of appearances\n",
    "event_appearances_transform = distil_event_appearances.copy()\n",
    "# event_appearances_transform.info()\n",
    "# event_appearances_transform.head(10)\n",
    "# event_appearances_transform.to_csv('event_appearances_transform.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_jobs = jobs.drop([\n",
    "    'permalink', # not useful\n",
    "    'cb_url', # not useful\n",
    "    'rank', # all null\n",
    "    'org_name', # there are null values, and we use uuid to make reference anyways\n",
    "    'created_at', # not useful\n",
    "    'updated_at', # not useful\n",
    "    'type', # not meaningful, they are all 'job'\n",
    "    'name', # full name only\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping\n",
    "jobs_drop = distil_jobs.copy()\n",
    "\n",
    "# according to job_type, the thrown ones are mostly employees, insignificant to our target(founder, ceo ...)\n",
    "# we are concern about founders' jobs & experiences\n",
    "jobs_drop = jobs_drop[jobs_drop['title'].notnull()]\n",
    "\n",
    "# remove illogical instances with started_on < ended_on\n",
    "jobs_drop = jobs_drop[~(jobs_drop['started_on'] > jobs_drop['ended_on'])]\n",
    "jobs_drop = jobs_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "jobs_fill = jobs_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming\n",
    "jobs_transform = jobs_fill.copy()\n",
    "\n",
    "#job types to boolean flags : is_executive, is_employee, etc.\n",
    "job_type_dummy = pd.get_dummies(jobs_transform['job_type'], prefix = 'is')\n",
    "jobs_transform = pd.concat((jobs_transform, job_type_dummy), axis = 1)\n",
    "\n",
    "#get job duration for both ended jobs and current job (in days)\n",
    "duration_ended = jobs_transform[jobs_transform['ended_on'].notnull()]['ended_on'] - jobs_transform[jobs_transform['ended_on'].notnull()]['started_on']\n",
    "duration_ended = pd.DataFrame({'duration_ended': duration_ended})\n",
    "jobs_transform = pd.concat((jobs_transform, duration_ended), axis = 1)\n",
    "jobs_transform = jobs_transform.reset_index(drop=True)\n",
    "\n",
    "duration_current = jobs_transform[jobs_transform['ended_on'].isnull() & jobs_transform['started_on'].notnull()]['started_on']\n",
    "duration_current = duration_current.apply(func=get_age_operating) # same method with getting company's age earlier\n",
    "duration_current = pd.DataFrame({'duration_current': duration_current})\n",
    "jobs_transform = pd.concat((jobs_transform, duration_current), axis = 1)\n",
    "jobs_transform = jobs_transform.reset_index(drop=True)\n",
    "\n",
    "# is_founder, founder is not a type of job but contain in job title. All jobs with string 'founder', 'co-founder' etc will have value of 1 in is_founder\n",
    "jobs_transform['is_founder'] = jobs_transform['title'].where(jobs_transform['title'].str.contains(\"founder|Founder\")).notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking:\n",
    "# jobs_transform.info()\n",
    "# jobs_transform.head(10)\n",
    "# jobs_transform.to_csv('jobs_transform.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. org_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_org_parents = org_parents.drop([\n",
    "    'permalink',\n",
    "    'cb_url', # not useful\n",
    "    'rank', # available in distil_organizations\n",
    "    'type', # not meaningful, they all are 'organization'\n",
    "    'created_at', # not useful\n",
    "    'updated_at' # not useful\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking:\n",
    "# no processing needed in this stage. \n",
    "org_parents_transform = distil_org_parents.copy()\n",
    "# org_parents_transform.info()\n",
    "# org_parents_transform.head(10)\n",
    "# org_parents_transform.to_csv('org_parents_transform.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_people = people.drop([\n",
    "    'permalink', # not useful\n",
    "    'cb_url',# not useful\n",
    "    'logo_url',# not useful\n",
    "    'type', # not useful, they are all 'person'\n",
    "    'created_at',# not useful\n",
    "    'updated_at', # not useful\n",
    "    'region', # too much granularity\n",
    "    'city', # too much granularity, we already have country & state code\n",
    "    'rank', # not useful\n",
    "    'name', # not useful, basically first name + last name\n",
    "    'featured_job_organization_name' # we use uuid to make reference anyways\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping\n",
    "people_drop = distil_people.copy()\n",
    "\n",
    "# drop insignificant instances without name\n",
    "people_drop = people_drop[people_drop['first_name'].notnull()]\n",
    "people_drop = people_drop[people_drop['last_name'].notnull()]\n",
    "people_drop = people_drop.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "people_fill = people_drop.copy()\n",
    "\n",
    "#create new category for unknowns, easier management\n",
    "people_fill['country_code'] = people_fill['country_code'].fillna('Unknown')\n",
    "people_fill['state_code'] = people_fill['state_code'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming\n",
    "people_transform = people_fill.copy()\n",
    "\n",
    "# like organition, social media convert to number_of_social_media_url\n",
    "people_transform['has_facebook'] = people_transform['facebook_url'].notnull().astype(int)\n",
    "people_transform['has_linkedin'] = people_transform['linkedin_url'].notnull().astype(int)\n",
    "people_transform['has_twitter'] = people_transform['twitter_url'].notnull().astype(int)\n",
    "people_transform['num_social_media_url'] = people_transform['has_facebook']+people_transform['has_linkedin']+people_transform['has_twitter']\n",
    "\n",
    "# convert gender to is_male and is_female\n",
    "# there are too many entries with gender neither F/M, too costly to process so they are omitted.\n",
    "genders = pd.get_dummies(people_transform['gender'], prefix='is')\n",
    "people_transform[['is_male','is_female']] = genders[['is_male','is_female']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_transform = people_transform.drop([\n",
    "    'facebook_url',\n",
    "    'linkedin_url',\n",
    "    'twitter_url',\n",
    "    'gender' # converted to boolean flags\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for checking:\n",
    "# people_transform.info()\n",
    "# people_transform.head(10)\n",
    "# people_transform.to_csv('people_transform.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Concatenating org & managerial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### organizations\n",
    "- org_parents_transform -> add parent_uuid, parent_name\n",
    "- event_appearances -> add num_event_appearances_org\n",
    "- jobs -> num_total_jobs, num_current_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_concat = organizations_transform.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating org_parents\n",
    "\n",
    "#-> add parent to organization, including parent name and parent uuid, and a boolean flag to signify if it has a parent\n",
    "organizations_concat = organizations_concat.merge(org_parents_transform.drop(['name'],axis=1),how='left', on='uuid')\n",
    "organizations_concat['has_parent'] = organizations_concat['parent_uuid'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating event_appearances\n",
    "\n",
    "# count num_event_appearances_org in all time\n",
    "temp = event_appearances_transform[event_appearances_transform['participant_type']=='organization'].copy()\n",
    "temp['num_event_appearances_org'] = 1\n",
    "num_event_appearances_org = temp.groupby(['participant_uuid']).sum() # a dataframe with parti_uuid as unique entries and a column of frequency\n",
    "num_event_appearances_org.index.name = 'uuid'\n",
    "organizations_concat = organizations_concat.merge(num_event_appearances_org,how='left', on='uuid') # add the column to main org dataframe\n",
    "organizations_concat['num_event_appearances_org'] = organizations_concat['num_event_appearances_org'].fillna(0) # if null meaning no event_appearances available = 0 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating jobs\n",
    "\n",
    "# num_total_jobs: historically how many jobs of a company has been posted on crunchbase\n",
    "num_total_jobs = pd.DataFrame(jobs_transform['org_uuid'].value_counts()) # counting how many times does a org's uuid appear = how many jobs it has\n",
    "num_total_jobs = num_total_jobs.rename(columns={\"org_uuid\":\"num_total_jobs\"})\n",
    "num_total_jobs.index.name = 'uuid'\n",
    "\n",
    "organizations_concat = organizations_concat.merge(num_total_jobs,how='left', on='uuid')\n",
    "organizations_concat['num_total_jobs'] = organizations_concat['num_total_jobs'].fillna(0)\n",
    "\n",
    "# num_current_jobs： historically how many jobs of a company has been posted on crunchbase and they are current\n",
    "num_current_jobs = pd.DataFrame(jobs_transform[jobs_transform['is_current']==1]['org_uuid'].value_counts())\n",
    "num_current_jobs = num_current_jobs.rename(columns={\"org_uuid\":\"num_current_jobs\"})\n",
    "num_current_jobs.index.name = 'uuid'\n",
    "\n",
    "organizations_concat = organizations_concat.merge(num_current_jobs,how='left', on='uuid') # add the 2 columns to main org dataframe\n",
    "organizations_concat['num_current_jobs'] = organizations_concat['num_current_jobs'].fillna(0) # if missing = 0 jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizations_concat.head()\n",
    "# organizations_concat.to_csv('organizations_concat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### people \n",
    "- event_appearances -> add num_event_appearances_person\n",
    "- degrees -> add num_completed_degrees, num_incomplete_degrees, num_completed_degrees_from_top_500_schools, num_incomplete_degrees_from_top_500_schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_concat = people_transform.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating event_appearances\n",
    "\n",
    "# count num_event_appearances_person\n",
    "num_event_appearances_person = pd.DataFrame(event_appearances_transform[event_appearances_transform['participant_type']=='person']['participant_uuid'].value_counts()) # counting how many times does a person's uuid appear = how many times he appeared\n",
    "num_event_appearances_person = num_event_appearances_person.rename(columns={\"participant_uuid\":\"num_event_appearances_person\"})\n",
    "num_event_appearances_person.index.name = 'uuid'\n",
    "people_concat = people_concat.merge(num_event_appearances_person,how='left', on='uuid') # add the column to people dataframe\n",
    "people_concat['num_event_appearances_person'] = people_concat['num_event_appearances_person'].fillna(0) # if null meaning no event_appearances available = 0 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating degrees\n",
    "\n",
    "# num_completed_degrees\n",
    "num_completed_degrees = pd.DataFrame(degrees_transform[degrees_transform['is_completed']==1]['person_uuid'].value_counts()) # counting how many times does a person's uuid appear = how many degrees he holds \n",
    "num_completed_degrees.index.name = 'uuid'\n",
    "people_concat = people_concat.merge(num_completed_degrees.rename(columns={\"person_uuid\":\"num_completed_degrees\"}),how='left', on='uuid') # add the column to people dataframe\n",
    "people_concat['num_completed_degrees'] = people_concat['num_completed_degrees'].fillna(0)\n",
    "\n",
    "# num_incomplete_degrees\n",
    "num_incomplete_degrees = pd.DataFrame(degrees_transform[degrees_transform['is_completed']==0]['person_uuid'].value_counts()) # same logic\n",
    "num_incomplete_degrees.index.name = 'uuid'\n",
    "people_concat = people_concat.merge(num_incomplete_degrees.rename(columns={\"person_uuid\":\"num_incomplete_degrees\"}),how='left', on='uuid') # add the column to people dataframe\n",
    "people_concat['num_incomplete_degrees'] = people_concat['num_incomplete_degrees'].fillna(0)\n",
    "\n",
    "# num_completed_degrees_from_top_500_schools\n",
    "completed_degrees = degrees_transform[degrees_transform['is_completed']==1]\n",
    "num_completed_degrees_from_top_500_schools = pd.DataFrame(completed_degrees[completed_degrees['is_from_top_500_schools']==1]['person_uuid'].value_counts()) # same logic\n",
    "num_completed_degrees_from_top_500_schools.index.name = 'uuid'\n",
    "people_concat = people_concat.merge(num_completed_degrees_from_top_500_schools.rename(columns={\"person_uuid\":\"num_completed_degrees_from_top_500_schools\"}),how='left', on='uuid') # add the column to people dataframe\n",
    "people_concat['num_completed_degrees_from_top_500_schools'] = people_concat['num_completed_degrees_from_top_500_schools'].fillna(0)\n",
    "\n",
    "# num_incomplete_degrees_from_top_500_schools\n",
    "incomplete_degrees = degrees_transform[degrees_transform['is_completed']==0]\n",
    "num_incomplete_degrees_from_top_500_schools = pd.DataFrame(incomplete_degrees[incomplete_degrees['is_from_top_500_schools']==1]['person_uuid'].value_counts()) # same logic\n",
    "num_incomplete_degrees_from_top_500_schools.index.name = 'uuid'\n",
    "people_concat = people_concat.merge(num_incomplete_degrees_from_top_500_schools.rename(columns={\"person_uuid\":\"num_incomplete_degrees_from_top_500_schools\"}),how='left', on='uuid') # add the column to people dataframe\n",
    "people_concat['num_incomplete_degrees_from_top_500_schools'] = people_concat['num_incomplete_degrees_from_top_500_schools'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_concat.head()\n",
    "# people_concat.to_csv('people_concat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jobs\n",
    "- org -> FAANG_experience\n",
    "- people -> link all features here so it's easier to evaluate a firm's cumulative employee background by entries in jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_concat = jobs_transform.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating people\n",
    "\n",
    "# taking the meaningful & transformed columns in people_concat\n",
    "jobs_concat = jobs_concat.merge(people_concat[['has_facebook',\n",
    "                                               'has_linkedin',\n",
    "                                               'has_twitter',\n",
    "                                               'uuid',\n",
    "                                               'num_social_media_url',\n",
    "                                               'num_event_appearances_person',\n",
    "                                               'num_completed_degrees', \n",
    "                                               'num_incomplete_degrees',\n",
    "                                               'num_completed_degrees_from_top_500_schools',\n",
    "                                                'num_incomplete_degrees_from_top_500_schools',\n",
    "                                               'is_male',\n",
    "                                               'is_female']].rename(columns={'uuid':'person_uuid'}),on='person_uuid')\n",
    "\n",
    "# identify male and female founders\n",
    "jobs_concat['is_male_founder']=jobs_concat['is_male']&jobs_concat['is_founder']\n",
    "jobs_concat['is_female_founder']=jobs_concat['is_female']&jobs_concat['is_founder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda3\\envs\\TEMG4952A\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\emily\\Anaconda3\\envs\\TEMG4952A\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "# creating has_FAANG_exp\n",
    "\n",
    "'''\n",
    "df662812-7f97-0b43-9d3e-12f64f504fbb   Facebook (unique in organization)\n",
    "7063d087-96b8-2cc1-ee88-c221288acc2a   Apple (unique in organization)\n",
    "3a7ec450-5422-1553-6c6a-4b28f6d4a17c   Netflix (unique in organization)\n",
    "\n",
    "Amazon: 46 instances in org with employees>10\n",
    "organizations_transform[organizations_transform['name'].str.contains('Amazon')].sort_values('employee_count')[['employee_count','is_operating','name']][organizations_transform['employee_count']!=0]\n",
    "\n",
    "Google: 19 instances in org with employees>10\n",
    "organizations_transform[organizations_transform['name'].str.contains('Google')][organizations_transform['employee_count']!=0]\n",
    "\n",
    "'''\n",
    "# create is_FAANG for jobs\n",
    "FAANG_uuid = organizations_transform[organizations_transform['name'].str.contains('Google')][organizations_transform['employee_count']!=0]['uuid'] # extracting Google's uuids\n",
    "FAANG_uuid = FAANG_uuid.append(organizations_transform[organizations_transform['name'].str.contains('Amazon')][organizations_transform['employee_count']!=0]['uuid']) # extracting Amazon's uuids\n",
    "FAANG_uuid = FAANG_uuid.append(pd.Series(['df662812-7f97-0b43-9d3e-12f64f504fbb','7063d087-96b8-2cc1-ee88-c221288acc2a','3a7ec450-5422-1553-6c6a-4b28f6d4a17c'])) # appeding FB, Apple, Netflix uuids\n",
    "FAANG_uuid = pd.DataFrame({'is_FAANG':1, 'org_uuid':FAANG_uuid})\n",
    "\n",
    "jobs_concat = jobs_concat.merge(FAANG_uuid, how='left', on='org_uuid') # link value is_FAANG = 1 for those jobs with organizations in FAANG. \n",
    "jobs_concat['is_FAANG'] = jobs_concat['is_FAANG'].fillna(0) # if no '1', then it is not FAANG -> 0\n",
    "\n",
    "# create has_FAANG_exp_person for people\n",
    "has_FAANG_exp_person = jobs_concat[['person_uuid','is_FAANG']].groupby(['person_uuid']).sum() # identify what people has FAANG exp and the number stands for # of FAANG jobs\n",
    "has_FAANG_exp_person = has_FAANG_exp_person.rename(columns={'is_FAANG': 'has_FAANG_exp_person'})\n",
    "\n",
    "# create has_FAANG_exp_person for jobs\n",
    "jobs_concat = jobs_concat.merge(has_FAANG_exp_person, how='left', on='person_uuid') # link person info to jobs\n",
    "\n",
    "# create has_FAANG_exp_founder for organizations\n",
    "jobs_concat['has_FAANG_exp_founder'] = jobs_concat['is_founder'] & jobs_concat['has_FAANG_exp_person'] # for job entries that are 'founder' and the person has_FAANG_exp, we create a boolean flag for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_concat.info()\n",
    "# jobs_concat.to_csv('jobs_concat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### people + organizations + jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group jobs_concat by org, we are essentially evaluating the cumulative features of each company.\n",
    "temp = jobs_concat.groupby(['org_uuid']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the useful features only and rename them since they are 'sums' now and are feature or the orgamization.\n",
    "temp = pd.DataFrame({'num_male_founder':temp['is_male_founder'],\n",
    "                     'num_female_founder':temp['is_female_founder'],\n",
    "                     'num_event_appearances_employee':temp['num_event_appearances_person'],\n",
    "                     'num_completed_degrees_employee':temp['num_completed_degrees'],\n",
    "                     'num_incomplete_degrees_employee':temp['num_incomplete_degrees'],\n",
    "                     'num_completed_degrees_from_top_500_schools':temp['num_completed_degrees_from_top_500_schools'],\n",
    "                     'num_incomplete_degrees_from_top_500_schools':temp['num_incomplete_degrees_from_top_500_schools'],\n",
    "                     'num_FAANG_exp_founder':temp['has_FAANG_exp_founder'],\n",
    "                    'percentage_of_male_founder': temp['is_male_founder'] / (temp['is_male_founder']+temp['is_female_founder']), # calculate % of male & female founders, so it doesnt get affected by size of company\n",
    "                    'percentage_of_female_founder': temp['is_female_founder'] / (temp['is_male_founder']+temp['is_female_founder']),\n",
    "                      })\n",
    "temp.index.name = 'uuid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_p1 = organizations_concat.merge(temp, on='uuid') # put the newly dericed features related to founders background, managerial features to the main org dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizations_p1.info()\n",
    "# organizations_p1.to_csv('organizations_concat_p1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2 Preprocessing: funding rounds related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_acquisitions = acquisitions.drop([\n",
    "    'type',\n",
    "    'created_at', # not useful\n",
    "    'updated_at',\n",
    "    'permalink',\n",
    "    'cb_url',\n",
    "    'acquiree_cb_url',\n",
    "    'acquirer_cb_url',\n",
    "    'acquiree_region', # too detailed---\n",
    "    'acquiree_city',\n",
    "    'price',\n",
    "    'price_currency_code',\n",
    "    'rank',\n",
    "    'acquirer_region',\n",
    "    'acquirer_city',\n",
    "    'acquisition_type',\n",
    "    'uuid',  # since uuid of the transaction\n",
    "    'acquiree_name', # extra feature during cocatenation of dataframes\n",
    "    'name', # irrelevant during concatenation,\n",
    "    'acquiree_country_code', # extra during concatenation\n",
    "    'acquiree_state_code' # extra during concatenation\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling & transform\n",
    "acquisitions_transform = distil_acquisitions.copy()\n",
    "\n",
    "#missing values for state_code's and country_code's, price_usd--> not initializing it to 0 --> not needed for acquiree after concatenation\n",
    "acquisitions_transform['acquirer_country_code']= acquisitions_transform['acquirer_country_code'].fillna(0) #if country_code=NaN, then just fill 0\n",
    "acquisitions_transform['acquirer_state_code']=acquisitions_transform['acquirer_state_code'].fillna(0) #if state_code= NaN, then just fill 0\n",
    "\n",
    "#create BOOLEAN FLAG for is_acquisition_price\n",
    "acquisitions_transform['is_acquisition_price']= np.where(acquisitions_transform['price_usd'].notnull(),1,0) # 1 for share price, 0 for no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquisitions_transform.info(verbose=True)\n",
    "# acquisitions_transform.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. funding_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_funding_rounds = funding_rounds.drop([\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "    'permalink',\n",
    "    'cb_url',\n",
    "    'state_code',\n",
    "    'region',\n",
    "    'city',\n",
    "    'post_money_valuation', # we use post_money_valuation_usd\n",
    "    'post_money_valuation_currency_code',\n",
    "    'rank',\n",
    "    'raised_amount',\n",
    "    'raised_amount_currency_code',\n",
    "    'type', # only one type='funding_rounds',\n",
    "    'name', # name of investment is irrelevant\n",
    "    'org_name',\n",
    "    'country_code', # duplicate during concatenation\n",
    "    'lead_investor_uuids' # irrelevant\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling & transform\n",
    "funding_rounds_transform = distil_funding_rounds.copy()\n",
    "\n",
    "# drop the rows with no raised_amount_usd (366,609 instances originally -> 267271 afterwards)\n",
    "funding_rounds_transform.drop(funding_rounds_transform[funding_rounds_transform['raised_amount_usd'].isnull()].index, inplace=True) \n",
    "\n",
    "# creating BOOLEAN FLAGS to identify which stage of the funding round belongs to\n",
    "# 1) is_PreSeries= seed, grant, angel, pre-seed, equity_crowdfunding, private_equity, undisclosed, corporate_round, product_crowdfunding, non_equity_assistance\n",
    "funding_rounds_transform['is_PreSeries']= np.where(funding_rounds_transform['investment_type'].str.contains('seed|grant|angel|pre_seed|equity_crowdfunding|private_equity|undisclosed|corporate_round|product_crowdfunding|non_equity_assistance'),1,0) # 1 for true, 0 for false\n",
    "\n",
    "# 2) is_RoundA = series_a\n",
    "funding_rounds_transform['is_RoundA']= np.where(funding_rounds_transform['investment_type'].str.contains('series_a'),1,0)\n",
    "\n",
    "# 3) is_RoundB= series_b\n",
    "funding_rounds_transform['is_RoundB']= np.where(funding_rounds_transform['investment_type'].str.contains('series_b'),1,0)\n",
    "\n",
    "# 4) is_RoundC= series_c\n",
    "funding_rounds_transform['is_RoundC']= np.where(funding_rounds_transform['investment_type'].str.contains('series_c'),1,0)\n",
    "\n",
    "#5) is_RoundD= series_d, series_unknown, convertible_note, series_e, series_f, series_g, series_h, series_i, series_j, \n",
    "funding_rounds_transform['is_RoundD']= np.where(funding_rounds_transform['investment_type'].str.contains('series_d|series_unknown|convertible_note|series_e|series_f|series_g|series_h|series_i|series_j'),1,0)                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funding_rounds_transform.info(verbose=True)\n",
    "# funding_rounds_transform.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_ipos = ipos.drop([\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "    'state_code',\n",
    "    'region',\n",
    "    'city',\n",
    "    'stock_exchange_symbol',\n",
    "    'share_price',\n",
    "    'share_price_currency_code',\n",
    "    'permalink',\n",
    "    'cb_url',\n",
    "    'org_cb_url',\n",
    "    'type',\n",
    "    'valuation_price',\n",
    "    'valuation_price_currency_code',\n",
    "    'money_raised',\n",
    "    'money_raised_currency_code',\n",
    "    'rank',\n",
    "    'name',\n",
    "    'uuid' #ipo uuid not relevant\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_transform = distil_ipos.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_investors =investors.drop([\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "    'permalink',\n",
    "    'cb_url',\n",
    "    'facebook_url',\n",
    "    'linkedin_url',\n",
    "    'twitter_url',\n",
    "    'logo_url',\n",
    "    'domain',\n",
    "    'state_code',\n",
    "    'region',\n",
    "    'city',\n",
    "    'total_funding',\n",
    "    'total_funding_currency_code',\n",
    "    'rank',\n",
    "    'type'  # just has type= 'organization' or 'person'\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "investors_transform = distil_investors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare top_500_investors for later to identify whether a firm is invested by top investors in a specific round\n",
    "investors_sort = investors_transform.sort_values('investment_count',ascending=False) # top investors are chosen based on number of investments made. Higher investment count give a higher rank.\n",
    "top_500_investors = investors_sort.head(500).reset_index()[['uuid','investment_count']] # after sorting, take the top 500 only\n",
    "top_500_investors['is_top_500_investors'] = 1\n",
    "top_500_investors = top_500_investors.rename(columns={'uuid':'investor_uuid'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_investments = investments.drop([\n",
    "    'created_at',\n",
    "    'updated_at',    \n",
    "    'permalink',\n",
    "    'cb_url',\n",
    "    'type', # only 'investment'\n",
    "    'rank',\n",
    "    'investor_type' #only has 2 types, 'person' and 'organization'\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "investments_transform = distil_investments.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using top_500_investors, identify i each investment entry is made by top investor (there could be multiple top investors in each funding round)\n",
    "is_top_500_investors = investments_transform[['funding_round_uuid','investor_uuid']].merge(top_500_investors, how='left',on='investor_uuid')\n",
    "is_top_500_investors = is_top_500_investors[['funding_round_uuid','is_top_500_investors']]\n",
    "# count number of top investos in each funding round\n",
    "num_top_500_investors = is_top_500_investors.groupby('funding_round_uuid').sum().rename(columns={'is_top_500_investors':'num_top_500_investors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2  - Concatenating funding rounds related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_concat_p2 = organizations_p1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acquisition\n",
    "- -> acquired_on, has_acquirer, acquirer_name, is_acquisition_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare acquisitions for concat\n",
    "acquisitions_concat = acquisitions_transform.copy()\n",
    "\n",
    "# some companies were acquired for multiple times -> keep the earliest acquirer only for easier processing\n",
    "acquisitions_concat = acquisitions_concat.sort_values('acquired_on').drop_duplicates('acquiree_uuid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funding_rounds\n",
    "- investors -> num_top_500_investors in each round, number of investors each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare funding_rounds for concat\n",
    "funding_rounds_concat = funding_rounds_transform.copy()\n",
    "\n",
    "# rename uuid as funding_rounds_uuid\n",
    "funding_rounds_concat = funding_rounds_concat.rename(columns={'uuid':'funding_round_uuid'})\n",
    "\n",
    "# concat num_top_500_investors to funding_rounds.\n",
    "funding_rounds_concat = funding_rounds_concat.merge(num_top_500_investors, how='left',on='funding_round_uuid')\n",
    "# if null values -> no top investors -> fill with 0 \n",
    "funding_rounds_concat['num_top_500_investors'] = funding_rounds_concat['num_top_500_investors'].fillna(0)\n",
    "\n",
    "# create num top investors for each round\n",
    "funding_rounds_concat['PreSeries_num_top_500_investors'] = funding_rounds_concat['is_PreSeries'] * funding_rounds_concat['num_top_500_investors']\n",
    "funding_rounds_concat['RoundA_num_top_500_investors'] = funding_rounds_concat['is_RoundA'] * funding_rounds_concat['num_top_500_investors']\n",
    "funding_rounds_concat['RoundB_num_top_500_investors'] = funding_rounds_concat['is_RoundB'] * funding_rounds_concat['num_top_500_investors']\n",
    "funding_rounds_concat['RoundC_num_top_500_investors'] = funding_rounds_concat['is_RoundC'] * funding_rounds_concat['num_top_500_investors']\n",
    "funding_rounds_concat['RoundD_num_top_500_investors'] = funding_rounds_concat['is_RoundD'] * funding_rounds_concat['num_top_500_investors']\n",
    "\n",
    "# rename columns so they make sense later during concatenation\n",
    "funding_rounds_concat = funding_rounds_concat.rename(columns={'org_uuid':'uuid','num_top_500_investors':'total_num_top_500_investors' })\n",
    "\n",
    "# for each funding round with raised_amount positive -> we assume there must be at least one investor\n",
    "funding_rounds_concat['investor_count']= funding_rounds_concat['investor_count'].fillna(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defin a helper function to help preparing features for each round - investors_count, announced_on, raised_amount_usd, post_money_valuation_usd\n",
    "def prep_round_feature(name):\n",
    "    funding_rounds_concat[name+'_investor_count']= np.where(funding_rounds_concat['is_'+name]==1, funding_rounds_concat['investor_count'],0)\n",
    "    funding_rounds_concat[name+'_announced_on']= np.where(funding_rounds_concat['is_'+name]==1, funding_rounds_concat['announced_on'],0)\n",
    "    funding_rounds_concat[name+'_raised_amount_usd']= np.where(funding_rounds_concat['is_'+name]==1, funding_rounds_concat['raised_amount_usd'],0)\n",
    "    funding_rounds_concat[name+'_post_money_valuation_usd']= np.where(funding_rounds_concat['is_'+name]==1, funding_rounds_concat['post_money_valuation_usd'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_round_feature('PreSeries')\n",
    "prep_round_feature('RoundA')\n",
    "prep_round_feature('RoundB')\n",
    "prep_round_feature('RoundC')\n",
    "prep_round_feature('RoundD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some companies have multiple rounds in one category (eg 2 times in roundC) -> we keep the date for the earliest round only\n",
    "funding_rounds_dates = funding_rounds_concat[['uuid','PreSeries_announced_on','RoundA_announced_on','RoundB_announced_on','RoundC_announced_on','RoundD_announced_on']]\n",
    "funding_rounds_dates = convert_datetime(funding_rounds_dates.replace(0, np.nan)) # 0 cannot be convert to datetime -> replace with NaN instead\n",
    "funding_rounds_dates = funding_rounds_dates.groupby('uuid').min() # min takes the smallest dates only\n",
    "\n",
    "# for companies have multiple rounds in one category (eg 2 times in roundC) -> features other than date would be the sum (eg. raised_amount = amount of first time + amount of second time)\n",
    "funding_rounds_sort = funding_rounds_concat.groupby('uuid').sum()\n",
    "\n",
    "# since sum() does not sum up dates, join the features+dates tgt\n",
    "funding_rounds_concat = funding_rounds_dates.join(funding_rounds_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipos\n",
    "-  -> dates, price, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare ipos for concat\n",
    "ipos_concat = ipos_transform.copy()\n",
    "ipos_concat = convert_datetime(ipos_concat)\n",
    "\n",
    "# keep the earlist ipos dates only \n",
    "ipos_dates = ipos_concat[['org_uuid','stock_symbol','went_public_on']]\n",
    "ipos_dates = ipos_dates.sort_values('went_public_on').drop_duplicates('org_uuid') # sort by date then drop duplicate, so we keep the earliest one\n",
    "\n",
    "# keep the largest prices only\n",
    "ipos_price = ipos_concat[['org_uuid','share_price_usd','valuation_price_usd','money_raised_usd']]\n",
    "ipos_price = ipos_price.groupby('org_uuid').max()\n",
    "\n",
    "# merge the unique dates and unique prices -> unique ipos entry for each company\n",
    "ipos_concat = ipos_dates.merge(ipos_price, how='left', on='org_uuid')\n",
    "ipos_concat = ipos_concat.replace(0,np.nan) # 0 in fact means not available here\n",
    "\n",
    "# boolean flags\n",
    "ipos_concat['is_share_price_usd']= np.where(ipos_concat['share_price_usd'].notnull(),1,0) # 1 for share price, 0 for no\n",
    "ipos_concat['is_valuation_price_usd']= np.where(ipos_concat['valuation_price_usd'].notnull(),1,0) # 1 for valuation, 0 for no\n",
    "ipos_concat['is_money_raised_usd']= np.where(ipos_concat['money_raised_usd'].notnull(),1,0) # 1 for money raised, 0 for no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### org + acquisitions + funding_rounds + ipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating acquisitions_concat along acquiree_uuid-->uuid\n",
    "\n",
    "# rename acquiree_uuid as uuid\n",
    "acquisitions_concat = acquisitions_concat.rename(columns= {\"acquiree_uuid\":\"uuid\"}) \n",
    "\n",
    "# concatenate\n",
    "organizations_concat_p2= organizations_concat_p2.merge(acquisitions_concat,how='left', on='uuid')\n",
    "\n",
    "# add boolean flags: has_acquirer and then fill missing values\n",
    "organizations_concat_p2['has_acquirer']= np.where(organizations_concat_p2['acquirer_name'].notnull(),1,0)  \n",
    "organizations_concat_p2['is_acquisition_price']= organizations_concat_p2['is_acquisition_price'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating funding_rounds_concat along org_uuid-->uuid\n",
    "                                                                                \n",
    "# concatenate\n",
    "organizations_concat_p2= organizations_concat_p2.merge(funding_rounds_concat,how='left', on='uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating ipos_transform along org_uuid-->uuid\n",
    "\n",
    "#rename acquiree_uuid as uuid\n",
    "ipos_concat= ipos_concat.rename(columns= {\"org_uuid\":\"uuid\"})\n",
    "                            \n",
    "#creat copy and then concatenate\n",
    "organizations_concat_p2 = organizations_concat_p2.merge(ipos_concat,how='left', on='uuid')\n",
    "\n",
    "#fill missing values\n",
    "organizations_concat_p2['is_share_price_usd']= organizations_concat_p2['is_share_price_usd'].fillna(0)\n",
    "organizations_concat_p2['is_valuation_price_usd']= organizations_concat_p2['is_valuation_price_usd'].fillna(0)\n",
    "organizations_concat_p2['is_money_raised_usd']= organizations_concat_p2['is_money_raised_usd'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizations_concat_p2.to_csv('organizations_concat_p2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Additional Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_concat_p3 = organizations_concat_p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Time taken between each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the founded year is too old, the time between founded & certain round is too long and it gives error.\n",
    "a = organizations_concat_p3[organizations_concat_p3['founded_on'].dt.year>1720] # so we calculate those after 1720 only\n",
    "PreSeries_from_founded = a[a['PreSeries_announced_on'].notnull()]['PreSeries_announced_on'].dt.date - a[a['PreSeries_announced_on'].notnull()]['founded_on'].dt.date\n",
    "PreSeries_from = pd.DataFrame({'PreSeries_from_founded': PreSeries_from_founded})\n",
    "organizations_concat_p3 = pd.concat((organizations_concat_p3, PreSeries_from), axis = 1)\n",
    "organizations_concat_p3 = organizations_concat_p3.reset_index(drop=True)\n",
    "\n",
    "# googled, this should be a false record\n",
    "organizations_concat_p3.loc[organizations_concat_p3['name']=='Arithmer', 'founded_on']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a helper function to calculate time between each round\n",
    "def time_A_from_B(A, B):\n",
    "    suffix2 = '_on' if B=='founded' else '_announced_on' \n",
    "    time = organizations_concat_p3[organizations_concat_p3[A+'_announced_on'].notnull()][A+'_announced_on'] - organizations_concat_p3[organizations_concat_p3[A+'_announced_on'].notnull()][B+suffix2]\n",
    "    df = pd.DataFrame({A+'_from_'+B: time})\n",
    "    new_org = pd.concat((organizations_concat_p3, df), axis = 1)\n",
    "    new_org = new_org.reset_index(drop=True)\n",
    "    return new_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of all periods\n",
    "organizations_concat_p3 = time_A_from_B('RoundA','founded')\n",
    "organizations_concat_p3 = time_A_from_B('RoundA','PreSeries')\n",
    "organizations_concat_p3 = time_A_from_B('RoundB','founded')\n",
    "organizations_concat_p3 = time_A_from_B('RoundB','PreSeries')\n",
    "organizations_concat_p3 = time_A_from_B('RoundB','RoundA')\n",
    "organizations_concat_p3 = time_A_from_B('RoundC','founded')\n",
    "organizations_concat_p3 = time_A_from_B('RoundC','PreSeries')\n",
    "organizations_concat_p3 = time_A_from_B('RoundC','RoundA')\n",
    "organizations_concat_p3 = time_A_from_B('RoundC','RoundB')\n",
    "organizations_concat_p3 = time_A_from_B('RoundD','founded')\n",
    "organizations_concat_p3 = time_A_from_B('RoundD','PreSeries')\n",
    "organizations_concat_p3 = time_A_from_B('RoundD','RoundA')\n",
    "organizations_concat_p3 = time_A_from_B('RoundD','RoundB')\n",
    "organizations_concat_p3 = time_A_from_B('RoundD','RoundC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. finding event_appearances between each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the essential columns for processing only\n",
    "ea = event_appearances.drop(columns=['name','type','cb_url','created_at','updated_at','short_description','rank']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda3\\envs\\TEMG4952A\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Since there is no date column provvided, extract dates from event link, they look like XX-Conference-2012-2012329-XXX\n",
    "ea['has_date'] = ea['permalink'].str.contains('-(\\d{7,8})-',regex=True)\n",
    "ea['date'] = ea['permalink'].str.extract('-(\\d{7,8})-')\n",
    "# some of them has year only\n",
    "ea['has_year'] = ea['permalink'].str.contains('-(\\d{4})-',regex=True)\n",
    "ea['year'] = ea['permalink'].str.extract('-(\\d{4})-')\n",
    "# turn them to number/datetime\n",
    "ea['year'] = pd.to_numeric(ea['year'])\n",
    "ea['date'] = pd.to_datetime(ea['date'],format='%Y%m%d', yearfirst=True,errors='coerce')\n",
    "ea.loc[ea['has_date'],'year'] = np.nan # remove years for the entries with a date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the entries with date/year available only for counting, also drop useful col\n",
    "ea_drop = ea[ea['has_date']|ea['has_year']].drop(columns=['uuid','permalink','event_uuid','event_name','appearance_type']).rename(columns={'participant_uuid':'uuid'}) #remove rows without any date/year\n",
    "# split to two subsets: organization, person\n",
    "ea_org = ea_drop[ea_drop['participant_type']=='organization']\n",
    "ea_person = ea_drop[ea_drop['participant_type']=='person']\n",
    "\n",
    "# concat dates for different rounds + event appearance with their corresponding date -> determine which period did the firm appear in event\n",
    "ea_concat = organizations_concat_p3[['uuid','name','founded_on','closed_on','last_funding_on','PreSeries_announced_on','RoundA_announced_on','RoundB_announced_on','RoundC_announced_on','RoundD_announced_on']].copy()\n",
    "ea_org = ea_org.merge(ea_concat, how='left', on='uuid')\n",
    "\n",
    "# create boolean to identify each event -> if they happen before specific round\n",
    "ea_org['PS_ea'] = ((ea_org.PreSeries_announced_on >= ea_org.date)|( ea_org.PreSeries_announced_on.dt.year>= ea_org.year))\n",
    "ea_org['RA_ea'] = (ea_org.RoundA_announced_on >= ea_org.date)|( ea_org.RoundA_announced_on.dt.year>= ea_org.year)\n",
    "ea_org['RB_ea'] = (ea_org.RoundB_announced_on >= ea_org.date)|( ea_org.RoundB_announced_on.dt.year>= ea_org.year)\n",
    "ea_org['RC_ea'] = (ea_org.RoundC_announced_on >= ea_org.date)|( ea_org.RoundC_announced_on.dt.year>= ea_org.year)\n",
    "ea_org['RD_ea'] = (ea_org.RoundD_announced_on >= ea_org.date)|( ea_org.RoundD_announced_on.dt.year>= ea_org.year)\n",
    "\n",
    "# group by org uuid -> sum = count of how many times of event appearances\n",
    "ea_sum = ea_org.groupby(by='uuid').sum().drop(['has_date','has_year','year'],axis=1)\n",
    "\n",
    "# concatenate\n",
    "organizations_concat_p3 = organizations_concat_p3.merge(ea_sum, how='left',on='uuid')\n",
    "\n",
    "# fill NaN\n",
    "organizations_concat_p3.loc[organizations_concat_p3.PreSeries_announced_on.isnull(), 'PS_ea'] = np.nan\n",
    "organizations_concat_p3.loc[organizations_concat_p3.RoundA_announced_on.isnull(), 'RA_ea'] = np.nan\n",
    "organizations_concat_p3.loc[organizations_concat_p3.RoundB_announced_on.isnull(), 'RB_ea'] = np.nan\n",
    "organizations_concat_p3.loc[organizations_concat_p3.RoundC_announced_on.isnull(), 'RC_ea'] = np.nan\n",
    "organizations_concat_p3.loc[organizations_concat_p3.RoundD_announced_on.isnull(), 'RD_ea'] = np.nan\n",
    "\n",
    "# rename the counts to proper names\n",
    "organizations_concat_p3 = organizations_concat_p3.rename(columns={'PS_ea':'PreSeries_num_ea_org',\n",
    "                                                                               'RA_ea':'RoundA_num_ea_org',\n",
    "                                                                               'RB_ea':'RoundB_num_ea_org',\n",
    "                                                                               'RC_ea':'RoundC_num_ea_org',\n",
    "                                                                               'RD_ea':'RoundD_num_ea_org'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 439847 entries, 0 to 439846\n",
      "Data columns (total 124 columns):\n",
      " #    Column                                       Dtype          \n",
      "---   ------                                       -----          \n",
      " 0    uuid                                         object         \n",
      " 1    name                                         object         \n",
      " 2    cb_url                                       object         \n",
      " 3    domain                                       object         \n",
      " 4    homepage_url                                 object         \n",
      " 5    country_code                                 object         \n",
      " 6    state_code                                   object         \n",
      " 7    category_list                                object         \n",
      " 8    category_groups_list                         object         \n",
      " 9    num_funding_rounds                           float64        \n",
      " 10   total_funding_usd                            float64        \n",
      " 11   founded_on                                   datetime64[ns] \n",
      " 12   last_funding_on                              datetime64[ns] \n",
      " 13   closed_on                                    datetime64[ns] \n",
      " 14   employee_count                               object         \n",
      " 15   facebook_url                                 object         \n",
      " 16   linkedin_url                                 object         \n",
      " 17   twitter_url                                  object         \n",
      " 18   num_exits                                    int32          \n",
      " 19   has_domain                                   bool           \n",
      " 20   is_company                                   uint8          \n",
      " 21   is_investor                                  uint8          \n",
      " 22   is_school                                    uint8          \n",
      " 23   is_primary_company                           uint8          \n",
      " 24   is_primary_investor                          uint8          \n",
      " 25   is_primary_school                            uint8          \n",
      " 26   is_acquired                                  uint8          \n",
      " 27   is_closed                                    uint8          \n",
      " 28   is_ipo                                       uint8          \n",
      " 29   is_operating                                 uint8          \n",
      " 30   age_closed                                   timedelta64[ns]\n",
      " 31   age_operating                                timedelta64[ns]\n",
      " 32   has_facebook                                 int32          \n",
      " 33   has_linkedin                                 int32          \n",
      " 34   has_twitter                                  int32          \n",
      " 35   num_social_media_url_org                     int32          \n",
      " 36   founded_on_year                              float64        \n",
      " 37   closed_on_year                               float64        \n",
      " 38   parent_uuid                                  object         \n",
      " 39   parent_name                                  object         \n",
      " 40   has_parent                                   bool           \n",
      " 41   num_event_appearances_org                    float64        \n",
      " 42   num_total_jobs                               float64        \n",
      " 43   num_current_jobs                             float64        \n",
      " 44   num_male_founder                             int64          \n",
      " 45   num_female_founder                           int64          \n",
      " 46   num_event_appearances_employee               float64        \n",
      " 47   num_completed_degrees_employee               float64        \n",
      " 48   num_incomplete_degrees_employee              float64        \n",
      " 49   num_completed_degrees_from_top_500_schools   float64        \n",
      " 50   num_incomplete_degrees_from_top_500_schools  float64        \n",
      " 51   num_FAANG_exp_founder                        int64          \n",
      " 52   percentage_of_male_founder                   float64        \n",
      " 53   percentage_of_female_founder                 float64        \n",
      " 54   acquirer_uuid                                object         \n",
      " 55   acquirer_name                                object         \n",
      " 56   acquirer_country_code                        object         \n",
      " 57   acquirer_state_code                          object         \n",
      " 58   acquired_on                                  object         \n",
      " 59   price_usd                                    float64        \n",
      " 60   is_acquisition_price                         float64        \n",
      " 61   has_acquirer                                 int32          \n",
      " 62   PreSeries_announced_on                       datetime64[ns] \n",
      " 63   RoundA_announced_on                          datetime64[ns] \n",
      " 64   RoundB_announced_on                          datetime64[ns] \n",
      " 65   RoundC_announced_on                          datetime64[ns] \n",
      " 66   RoundD_announced_on                          datetime64[ns] \n",
      " 67   raised_amount_usd                            float64        \n",
      " 68   post_money_valuation_usd                     float64        \n",
      " 69   investor_count                               float64        \n",
      " 70   is_PreSeries                                 float64        \n",
      " 71   is_RoundA                                    float64        \n",
      " 72   is_RoundB                                    float64        \n",
      " 73   is_RoundC                                    float64        \n",
      " 74   is_RoundD                                    float64        \n",
      " 75   total_num_top_500_investors                  float64        \n",
      " 76   PreSeries_num_top_500_investors              float64        \n",
      " 77   RoundA_num_top_500_investors                 float64        \n",
      " 78   RoundB_num_top_500_investors                 float64        \n",
      " 79   RoundC_num_top_500_investors                 float64        \n",
      " 80   RoundD_num_top_500_investors                 float64        \n",
      " 81   PreSeries_investor_count                     float64        \n",
      " 82   PreSeries_raised_amount_usd                  float64        \n",
      " 83   PreSeries_post_money_valuation_usd           float64        \n",
      " 84   RoundA_investor_count                        float64        \n",
      " 85   RoundA_raised_amount_usd                     float64        \n",
      " 86   RoundA_post_money_valuation_usd              float64        \n",
      " 87   RoundB_investor_count                        float64        \n",
      " 88   RoundB_raised_amount_usd                     float64        \n",
      " 89   RoundB_post_money_valuation_usd              float64        \n",
      " 90   RoundC_investor_count                        float64        \n",
      " 91   RoundC_raised_amount_usd                     float64        \n",
      " 92   RoundC_post_money_valuation_usd              float64        \n",
      " 93   RoundD_investor_count                        float64        \n",
      " 94   RoundD_raised_amount_usd                     float64        \n",
      " 95   RoundD_post_money_valuation_usd              float64        \n",
      " 96   stock_symbol                                 object         \n",
      " 97   went_public_on                               datetime64[ns] \n",
      " 98   share_price_usd                              float64        \n",
      " 99   valuation_price_usd                          float64        \n",
      " 100  money_raised_usd                             float64        \n",
      " 101  is_share_price_usd                           float64        \n",
      " 102  is_valuation_price_usd                       float64        \n",
      " 103  is_money_raised_usd                          float64        \n",
      " 104  PreSeries_from_founded                       timedelta64[ns]\n",
      " 105  RoundA_from_founded                          timedelta64[ns]\n",
      " 106  RoundA_from_PreSeries                        timedelta64[ns]\n",
      " 107  RoundB_from_founded                          timedelta64[ns]\n",
      " 108  RoundB_from_PreSeries                        timedelta64[ns]\n",
      " 109  RoundB_from_RoundA                           timedelta64[ns]\n",
      " 110  RoundC_from_founded                          timedelta64[ns]\n",
      " 111  RoundC_from_PreSeries                        timedelta64[ns]\n",
      " 112  RoundC_from_RoundA                           timedelta64[ns]\n",
      " 113  RoundC_from_RoundB                           timedelta64[ns]\n",
      " 114  RoundD_from_founded                          timedelta64[ns]\n",
      " 115  RoundD_from_PreSeries                        timedelta64[ns]\n",
      " 116  RoundD_from_RoundA                           timedelta64[ns]\n",
      " 117  RoundD_from_RoundB                           timedelta64[ns]\n",
      " 118  RoundD_from_RoundC                           timedelta64[ns]\n",
      " 119  PreSeries_num_ea_org                         float64        \n",
      " 120  RoundA_num_ea_org                            float64        \n",
      " 121  RoundB_num_ea_org                            float64        \n",
      " 122  RoundC_num_ea_org                            float64        \n",
      " 123  RoundD_num_ea_org                            float64        \n",
      "dtypes: bool(2), datetime64[ns](9), float64(56), int32(6), int64(3), object(21), timedelta64[ns](17), uint8(10)\n",
      "memory usage: 374.2+ MB\n"
     ]
    }
   ],
   "source": [
    "organizations_concat_p3.info(verbose=True)\n",
    "# organizations_concat_p3.to_csv('439k_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4 VGR calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. dataset selction (for modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = organizations_concat_p3.copy()\n",
    "dataset = dataset.drop([\n",
    "    'category_list',\n",
    "    'founded_on',\n",
    "    'last_funding_on',\n",
    "    'closed_on',\n",
    "    'acquired_on',\n",
    "    'age_operating',\n",
    "    'age_closed',\n",
    "    'founded_on_year',\n",
    "    'closed_on_year',\n",
    "    'parent_uuid',\n",
    "    'parent_name',\n",
    "    'employee_count', #these features are time-dependent and could not be used to predict (they are useful for identifying existing big companies only)\n",
    "], axis=1)\n",
    "\n",
    "# only choose those are primarily company. Others are investors/schools that UBS is not interested in\n",
    "dataset = dataset[dataset['is_primary_company'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use companies within UBS's interested categories\n",
    "categories_of_interest = [\n",
    "    'Financial Services',\n",
    "    'Lending and Investments',\n",
    "    'Payments',\n",
    "    'Artificial Intelligence',\n",
    "    'Data and Analytics',\n",
    "    'Platforms',\n",
    "    'Privacy and Security',\n",
    "]\n",
    "\n",
    "# get the categories a company belongs to \n",
    "dataset['category_groups_list'] = dataset['category_groups_list'].apply(lambda x: strip_uninterested_categories(x, categories_of_interest))\n",
    "category_groups_list = dataset['category_groups_list'].fillna('').apply(split_categories)\n",
    "categories_of_interest_getter = category_groups_list.apply(lambda x: any(y in x for y in categories_of_interest))\n",
    "dataset = dataset[categories_of_interest_getter]\n",
    "\n",
    "# one-hot encode the category the company belongs to\n",
    "category_map = dataset.category_groups_list.str.split(',', expand=True).stack()\n",
    "category_dummies = pd.get_dummies(category_map, prefix='is_category').groupby(level=0).sum()\n",
    "dataset = pd.concat((dataset, category_dummies), axis = 1).drop(['category_groups_list'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  fill in missing valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We evaluate a company's success by Valuation Growth Rate (VGR)\n",
    "VGR from t0 to t1 = (valuation at t1 / valuation at t0) ^ (1 / (t1-t0)) -1\n",
    "\n",
    "Given the sparse data in valuation, we fill in them by using raised_amount_usd\n",
    "\n",
    "1.  we calculate to average equity ratio from companies that has both raised_amount and post_valuation \n",
    "by get_equity_ratio() for each round.\n",
    "\n",
    "2.  Then if the post_valuation is empty, we fill it with (raised_amount / avg equity ratio)\n",
    "'''\n",
    "def get_equity_ratio(df, raised_amount_col, post_money_col):\n",
    "    funding_data = dataset[[raised_amount_col,post_money_col]]\n",
    "    funding_data[raised_amount_col] = funding_data[raised_amount_col].replace(0,np.nan)\n",
    "    funding_data[post_money_col] = funding_data[post_money_col].replace(0,np.nan)\n",
    "    funding_data = funding_data[funding_data[raised_amount_col].notnull() & funding_data[post_money_col].notnull()]\n",
    "\n",
    "    funding_data['equity_ratio'] = funding_data[raised_amount_col] / funding_data[post_money_col]\n",
    "    return funding_data['equity_ratio'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda3\\envs\\TEMG4952A\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\emily\\Anaconda3\\envs\\TEMG4952A\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "avg_preSeries_equity_ratio = get_equity_ratio(\n",
    "    dataset,\n",
    "    'PreSeries_raised_amount_usd',\n",
    "    'PreSeries_post_money_valuation_usd'\n",
    ")\n",
    "avg_roundA_equity_ratio = get_equity_ratio(\n",
    "    dataset,\n",
    "    'RoundA_raised_amount_usd',\n",
    "    'RoundA_post_money_valuation_usd'\n",
    ")\n",
    "avg_roundB_equity_ratio = get_equity_ratio(\n",
    "    dataset,\n",
    "    'RoundB_raised_amount_usd',\n",
    "    'RoundB_post_money_valuation_usd'\n",
    ")\n",
    "avg_roundC_equity_ratio = get_equity_ratio(\n",
    "    dataset,\n",
    "    'RoundC_raised_amount_usd',\n",
    "    'RoundC_post_money_valuation_usd'\n",
    ")\n",
    "avg_roundD_equity_ratio = get_equity_ratio(\n",
    "    dataset,\n",
    "    'RoundD_raised_amount_usd',\n",
    "    'RoundD_post_money_valuation_usd'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28094025523667615"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preSeries_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2755640808542754"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_roundA_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2262574455112157"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_roundB_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18094789325964744"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_roundC_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2127669927118955"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_roundD_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 as mentioned about, fill in valuation\n",
    "\n",
    "def fill_valuation(x1,x2,factor):\n",
    "    x2 = x1 / factor if (x2 is np.nan or x2 == 0) else x2\n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['PreSeries_post_money_valuation_usd_augmented'] = dataset.apply(lambda x: fill_valuation(\n",
    "    x['PreSeries_raised_amount_usd'],\n",
    "    x['PreSeries_post_money_valuation_usd'],\n",
    "    avg_preSeries_equity_ratio),axis=1)\n",
    "dataset['RoundA_post_money_valuation_usd_augmented'] = dataset.apply(lambda x: fill_valuation(\n",
    "    x['RoundA_raised_amount_usd'],\n",
    "    x['RoundA_post_money_valuation_usd'],\n",
    "    avg_roundA_equity_ratio),axis=1)\n",
    "dataset['RoundB_post_money_valuation_usd_augmented'] = dataset.apply(lambda x: fill_valuation(\n",
    "    x['RoundB_raised_amount_usd'],\n",
    "    x['RoundB_post_money_valuation_usd'],\n",
    "    avg_roundB_equity_ratio),axis=1)\n",
    "dataset['RoundC_post_money_valuation_usd_augmented'] = dataset.apply(lambda x: fill_valuation(\n",
    "    x['RoundC_raised_amount_usd'],\n",
    "    x['RoundC_post_money_valuation_usd'],\n",
    "    avg_roundC_equity_ratio),axis=1)\n",
    "dataset['RoundD_post_money_valuation_usd_augmented'] = dataset.apply(lambda x: fill_valuation(\n",
    "    x['RoundD_raised_amount_usd'],\n",
    "    x['RoundD_post_money_valuation_usd'],\n",
    "    avg_roundD_equity_ratio),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. calculate VGR, drop null entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGR from t0 to t1 = (valuation at t1 / valuation at t0) ^ (1 / (t1-t0)) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ROI' refers to VGR\n",
    "# get_ROI calculates VGR of all time (if data available)\n",
    "def get_ROI(pre_v, A_v, B_v, C_v, D_v, pre_a, A_a, B_a, C_a, D_a):\n",
    "        time_naught = None\n",
    "        time_prime = None\n",
    "        valuation_naught = None\n",
    "        valuation_prime = None\n",
    "        valuations = [pre_v, A_v, B_v, C_v, D_v]\n",
    "        dates = [pre_a, A_a, B_a, C_a, D_a]\n",
    "        for i in range(5):\n",
    "            if valuations[i] and dates[i]:\n",
    "                time_naught = dates[i]\n",
    "                valuation_naught = valuations[i]\n",
    "                break\n",
    "        for i in range(4,-1,-1):\n",
    "            if valuations[i] and dates[i]:\n",
    "                time_prime = dates[i]\n",
    "                valuation_prime = valuations[i]\n",
    "                break\n",
    "        if not (time_naught and time_prime) or (time_naught == time_prime):\n",
    "            return np.nan\n",
    "        num_years = (time_prime - time_naught).days/365\n",
    "        try:\n",
    "            roi = (valuation_prime / valuation_naught) ** (1/num_years) - 1\n",
    "            if not roi.imag == 0:\n",
    "                return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "        return roi.real # only return real number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ROI' refers to VGR\n",
    "# get_ROI_from_to calculates VGR from t0 to t1\n",
    "\n",
    "def get_ROI_from_to(from_v, to_v, from_a, to_a):\n",
    "    time_naught = from_a\n",
    "    time_prime = to_a\n",
    "    valuation_naught = from_v\n",
    "    valuation_prime = to_v\n",
    "    if not (time_naught and time_prime and valuation_naught and valuation_prime) or (time_naught == time_prime):\n",
    "        return np.nan\n",
    "    num_years = (time_prime - time_naught).days/365\n",
    "    try:\n",
    "        roi = (valuation_prime / valuation_naught) ** (1/num_years) - 1\n",
    "        if not roi.imag == 0:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "    return roi.real # only return real number\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all dates are in timestamp format\n",
    "dataset = convert_datetime(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All time ROI (VGR)\n",
    "dataset['all_time_roi'] = dataset.apply(lambda x: get_ROI(\n",
    "    x['PreSeries_post_money_valuation_usd_augmented'],\n",
    "    x['RoundA_post_money_valuation_usd_augmented'],\n",
    "    x['RoundB_post_money_valuation_usd_augmented'],\n",
    "    x['RoundC_post_money_valuation_usd_augmented'],\n",
    "    x['RoundD_post_money_valuation_usd_augmented'],\n",
    "    x['PreSeries_announced_on'],\n",
    "    x['RoundA_announced_on'],\n",
    "    x['RoundB_announced_on'],\n",
    "    x['RoundC_announced_on'],\n",
    "    x['RoundD_announced_on']\n",
    "),axis=1)\n",
    "\n",
    "dataset = dataset[dataset['all_time_roi'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3827"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from PreSeries to RoundA\n",
    "dataset['roi_from_PS_to_RA'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['PreSeries_post_money_valuation_usd_augmented'],\n",
    "    x['RoundA_post_money_valuation_usd_augmented'],\n",
    "    x['PreSeries_announced_on'],\n",
    "    x['RoundA_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_PS_to_RA'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from PreSeries to RoundB\n",
    "dataset['roi_from_PS_to_RB'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['PreSeries_post_money_valuation_usd_augmented'],\n",
    "    x['RoundB_post_money_valuation_usd_augmented'],\n",
    "    x['PreSeries_announced_on'],\n",
    "    x['RoundB_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_PS_to_RB'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from PreSeries to RoundC\n",
    "dataset['roi_from_PS_to_RC'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['PreSeries_post_money_valuation_usd_augmented'],\n",
    "    x['RoundC_post_money_valuation_usd_augmented'],\n",
    "    x['PreSeries_announced_on'],\n",
    "    x['RoundC_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_PS_to_RC'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2953"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from PreSeries to RoundD\n",
    "dataset['roi_from_PS_to_RD'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['PreSeries_post_money_valuation_usd_augmented'],\n",
    "    x['RoundD_post_money_valuation_usd_augmented'],\n",
    "    x['PreSeries_announced_on'],\n",
    "    x['RoundD_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_PS_to_RD'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from PreSeries to IPO\n",
    "dataset['roi_from_PS_to_IPO'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['PreSeries_post_money_valuation_usd_augmented'],\n",
    "    x['valuation_price_usd'],\n",
    "    x['PreSeries_announced_on'],\n",
    "    x['went_public_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_PS_to_IPO'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3408"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundA to RoundB\n",
    "dataset['roi_from_RA_to_RB'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundA_post_money_valuation_usd_augmented'],\n",
    "    x['RoundB_post_money_valuation_usd_augmented'],\n",
    "    x['RoundA_announced_on'],\n",
    "    x['RoundB_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RA_to_RB'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundA to RoundC\n",
    "dataset['roi_from_RA_to_RC'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundA_post_money_valuation_usd_augmented'],\n",
    "    x['RoundC_post_money_valuation_usd_augmented'],\n",
    "    x['RoundA_announced_on'],\n",
    "    x['RoundC_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RA_to_RC'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2310"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundA to RoundD\n",
    "dataset['roi_from_RA_to_RD'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundA_post_money_valuation_usd_augmented'],\n",
    "    x['RoundD_post_money_valuation_usd_augmented'],\n",
    "    x['RoundA_announced_on'],\n",
    "    x['RoundD_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RA_to_RD'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundA to IPO\n",
    "dataset['roi_from_RA_to_IPO'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundA_post_money_valuation_usd_augmented'],\n",
    "    x['valuation_price_usd'],\n",
    "    x['RoundA_announced_on'],\n",
    "    x['went_public_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RA_to_IPO'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundB to RoundC\n",
    "dataset['roi_from_RB_to_RC'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundB_post_money_valuation_usd_augmented'],\n",
    "    x['RoundC_post_money_valuation_usd_augmented'],\n",
    "    x['RoundB_announced_on'],\n",
    "    x['RoundC_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RB_to_RC'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundB to RoundD\n",
    "dataset['roi_from_RB_to_RD'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundB_post_money_valuation_usd_augmented'],\n",
    "    x['RoundD_post_money_valuation_usd_augmented'],\n",
    "    x['RoundB_announced_on'],\n",
    "    x['RoundD_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RB_to_RD'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundB to IPO\n",
    "dataset['roi_from_RB_to_IPO'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundB_post_money_valuation_usd_augmented'],\n",
    "    x['valuation_price_usd'],\n",
    "    x['RoundB_announced_on'],\n",
    "    x['went_public_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RB_to_IPO'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1191"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundC to RoundD\n",
    "dataset['roi_from_RC_to_RD'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundC_post_money_valuation_usd_augmented'],\n",
    "    x['RoundD_post_money_valuation_usd_augmented'],\n",
    "    x['RoundC_announced_on'],\n",
    "    x['RoundD_announced_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RC_to_RD'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundC to IPO\n",
    "dataset['roi_from_RC_to_IPO'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundC_post_money_valuation_usd_augmented'],\n",
    "    x['valuation_price_usd'],\n",
    "    x['RoundC_announced_on'],\n",
    "    x['went_public_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RC_to_IPO'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGR from RoundD to IPO\n",
    "dataset['roi_from_RD_to_IPO'] = dataset.apply(lambda x: get_ROI_from_to(\n",
    "    x['RoundD_post_money_valuation_usd_augmented'],\n",
    "    x['valuation_price_usd'],\n",
    "    x['RoundD_announced_on'],\n",
    "    x['went_public_on'],\n",
    "),axis=1)\n",
    "dataset['roi_from_RD_to_IPO'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8865 entries, 26 to 439282\n",
      "Data columns (total 139 columns):\n",
      " #    Column                                        Dtype          \n",
      "---   ------                                        -----          \n",
      " 0    uuid                                          object         \n",
      " 1    name                                          object         \n",
      " 2    cb_url                                        object         \n",
      " 3    domain                                        object         \n",
      " 4    homepage_url                                  object         \n",
      " 5    country_code                                  object         \n",
      " 6    state_code                                    object         \n",
      " 7    num_funding_rounds                            float64        \n",
      " 8    total_funding_usd                             float64        \n",
      " 9    facebook_url                                  object         \n",
      " 10   linkedin_url                                  object         \n",
      " 11   twitter_url                                   object         \n",
      " 12   num_exits                                     int32          \n",
      " 13   has_domain                                    bool           \n",
      " 14   is_company                                    uint8          \n",
      " 15   is_investor                                   uint8          \n",
      " 16   is_school                                     uint8          \n",
      " 17   is_primary_company                            uint8          \n",
      " 18   is_primary_investor                           uint8          \n",
      " 19   is_primary_school                             uint8          \n",
      " 20   is_acquired                                   uint8          \n",
      " 21   is_closed                                     uint8          \n",
      " 22   is_ipo                                        uint8          \n",
      " 23   is_operating                                  uint8          \n",
      " 24   has_facebook                                  int32          \n",
      " 25   has_linkedin                                  int32          \n",
      " 26   has_twitter                                   int32          \n",
      " 27   num_social_media_url_org                      int32          \n",
      " 28   has_parent                                    bool           \n",
      " 29   num_event_appearances_org                     float64        \n",
      " 30   num_total_jobs                                float64        \n",
      " 31   num_current_jobs                              float64        \n",
      " 32   num_male_founder                              int64          \n",
      " 33   num_female_founder                            int64          \n",
      " 34   num_event_appearances_employee                float64        \n",
      " 35   num_completed_degrees_employee                float64        \n",
      " 36   num_incomplete_degrees_employee               float64        \n",
      " 37   num_completed_degrees_from_top_500_schools    float64        \n",
      " 38   num_incomplete_degrees_from_top_500_schools   float64        \n",
      " 39   num_FAANG_exp_founder                         int64          \n",
      " 40   percentage_of_male_founder                    float64        \n",
      " 41   percentage_of_female_founder                  float64        \n",
      " 42   acquirer_uuid                                 object         \n",
      " 43   acquirer_name                                 object         \n",
      " 44   acquirer_country_code                         object         \n",
      " 45   acquirer_state_code                           object         \n",
      " 46   price_usd                                     float64        \n",
      " 47   is_acquisition_price                          float64        \n",
      " 48   has_acquirer                                  int32          \n",
      " 49   PreSeries_announced_on                        datetime64[ns] \n",
      " 50   RoundA_announced_on                           datetime64[ns] \n",
      " 51   RoundB_announced_on                           datetime64[ns] \n",
      " 52   RoundC_announced_on                           datetime64[ns] \n",
      " 53   RoundD_announced_on                           datetime64[ns] \n",
      " 54   raised_amount_usd                             float64        \n",
      " 55   post_money_valuation_usd                      float64        \n",
      " 56   investor_count                                float64        \n",
      " 57   is_PreSeries                                  float64        \n",
      " 58   is_RoundA                                     float64        \n",
      " 59   is_RoundB                                     float64        \n",
      " 60   is_RoundC                                     float64        \n",
      " 61   is_RoundD                                     float64        \n",
      " 62   total_num_top_500_investors                   float64        \n",
      " 63   PreSeries_num_top_500_investors               float64        \n",
      " 64   RoundA_num_top_500_investors                  float64        \n",
      " 65   RoundB_num_top_500_investors                  float64        \n",
      " 66   RoundC_num_top_500_investors                  float64        \n",
      " 67   RoundD_num_top_500_investors                  float64        \n",
      " 68   PreSeries_investor_count                      float64        \n",
      " 69   PreSeries_raised_amount_usd                   float64        \n",
      " 70   PreSeries_post_money_valuation_usd            float64        \n",
      " 71   RoundA_investor_count                         float64        \n",
      " 72   RoundA_raised_amount_usd                      float64        \n",
      " 73   RoundA_post_money_valuation_usd               float64        \n",
      " 74   RoundB_investor_count                         float64        \n",
      " 75   RoundB_raised_amount_usd                      float64        \n",
      " 76   RoundB_post_money_valuation_usd               float64        \n",
      " 77   RoundC_investor_count                         float64        \n",
      " 78   RoundC_raised_amount_usd                      float64        \n",
      " 79   RoundC_post_money_valuation_usd               float64        \n",
      " 80   RoundD_investor_count                         float64        \n",
      " 81   RoundD_raised_amount_usd                      float64        \n",
      " 82   RoundD_post_money_valuation_usd               float64        \n",
      " 83   stock_symbol                                  object         \n",
      " 84   went_public_on                                datetime64[ns] \n",
      " 85   share_price_usd                               float64        \n",
      " 86   valuation_price_usd                           float64        \n",
      " 87   money_raised_usd                              float64        \n",
      " 88   is_share_price_usd                            float64        \n",
      " 89   is_valuation_price_usd                        float64        \n",
      " 90   is_money_raised_usd                           float64        \n",
      " 91   PreSeries_from_founded                        timedelta64[ns]\n",
      " 92   RoundA_from_founded                           timedelta64[ns]\n",
      " 93   RoundA_from_PreSeries                         timedelta64[ns]\n",
      " 94   RoundB_from_founded                           timedelta64[ns]\n",
      " 95   RoundB_from_PreSeries                         timedelta64[ns]\n",
      " 96   RoundB_from_RoundA                            timedelta64[ns]\n",
      " 97   RoundC_from_founded                           timedelta64[ns]\n",
      " 98   RoundC_from_PreSeries                         timedelta64[ns]\n",
      " 99   RoundC_from_RoundA                            timedelta64[ns]\n",
      " 100  RoundC_from_RoundB                            timedelta64[ns]\n",
      " 101  RoundD_from_founded                           timedelta64[ns]\n",
      " 102  RoundD_from_PreSeries                         timedelta64[ns]\n",
      " 103  RoundD_from_RoundA                            timedelta64[ns]\n",
      " 104  RoundD_from_RoundB                            timedelta64[ns]\n",
      " 105  RoundD_from_RoundC                            timedelta64[ns]\n",
      " 106  PreSeries_num_ea_org                          float64        \n",
      " 107  RoundA_num_ea_org                             float64        \n",
      " 108  RoundB_num_ea_org                             float64        \n",
      " 109  RoundC_num_ea_org                             float64        \n",
      " 110  RoundD_num_ea_org                             float64        \n",
      " 111  is_category_Artificial Intelligence           uint8          \n",
      " 112  is_category_Data and Analytics                uint8          \n",
      " 113  is_category_Financial Services                uint8          \n",
      " 114  is_category_Lending and Investments           uint8          \n",
      " 115  is_category_Payments                          uint8          \n",
      " 116  is_category_Platforms                         uint8          \n",
      " 117  is_category_Privacy and Security              uint8          \n",
      " 118  PreSeries_post_money_valuation_usd_augmented  float64        \n",
      " 119  RoundA_post_money_valuation_usd_augmented     float64        \n",
      " 120  RoundB_post_money_valuation_usd_augmented     float64        \n",
      " 121  RoundC_post_money_valuation_usd_augmented     float64        \n",
      " 122  RoundD_post_money_valuation_usd_augmented     float64        \n",
      " 123  all_time_roi                                  float64        \n",
      " 124  roi_from_PS_to_RA                             float64        \n",
      " 125  roi_from_PS_to_RB                             float64        \n",
      " 126  roi_from_PS_to_RC                             float64        \n",
      " 127  roi_from_PS_to_RD                             float64        \n",
      " 128  roi_from_PS_to_IPO                            float64        \n",
      " 129  roi_from_RA_to_RB                             float64        \n",
      " 130  roi_from_RA_to_RC                             float64        \n",
      " 131  roi_from_RA_to_RD                             float64        \n",
      " 132  roi_from_RA_to_IPO                            float64        \n",
      " 133  roi_from_RB_to_RC                             float64        \n",
      " 134  roi_from_RB_to_RD                             float64        \n",
      " 135  roi_from_RB_to_IPO                            float64        \n",
      " 136  roi_from_RC_to_RD                             float64        \n",
      " 137  roi_from_RC_to_IPO                            float64        \n",
      " 138  roi_from_RD_to_IPO                            float64        \n",
      "dtypes: bool(2), datetime64[ns](6), float64(75), int32(6), int64(3), object(15), timedelta64[ns](15), uint8(17)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part5 Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABEL = 5\n",
    "lb_make = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data = convert_datetime(bin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns are dropped for the modelling dataset. Commented so they don't give error\n",
    "\n",
    "# bin_data['founded_on_binned'] = pd.qcut(bin_data['founded_on'], q=NUM_LABEL,precision = 0)\n",
    "# bin_data['last_funding_on_binned'] = pd.qcut(bin_data['last_funding_on'], q=NUM_LABEL,precision = 0)\n",
    "# bin_data['closed_on_binned'] = pd.qcut(bin_data['closed_on'], q=NUM_LABEL,precision = 0)\n",
    "# bin_data['age_closed_binned'] = pd.qcut(bin_data['age_closed'], q=NUM_LABEL,precision = 0)\n",
    "# bin_data['age_operating_binned'] = pd.qcut(bin_data['age_operating'], q=NUM_LABEL,precision = 0)\n",
    "# bin_data['acquired_on_binned'] = pd.qcut(bin_data['acquired_on'], q=NUM_LABEL,precision = 0)\n",
    "\n",
    "# for this version, null values -> biggest bin (bin 5)\n",
    "# bin_data['founded_on_binned'] = lb_make.fit_transform(bin_data['founded_on_binned'])\n",
    "# bin_data['last_funding_on_binned'] = lb_make.fit_transform(bin_data['last_funding_on_binned'])\n",
    "# bin_data['closed_on_binned'] = lb_make.fit_transform(bin_data['closed_on_binned'])\n",
    "# bin_data['age_closed_binned'] = lb_make.fit_transform(bin_data['age_closed_binned'])\n",
    "# bin_data['age_operating_binned'] = lb_make.fit_transform(bin_data['age_operating_binned'])\n",
    "# bin_data['acquired_on_binned'] = lb_make.fit_transform(bin_data['acquired_on_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the time taken for each round for classification\n",
    "bin_data['PreSeries_from_founded_binned'] = pd.qcut(bin_data['PreSeries_from_founded'], q=NUM_LABEL,precision = 0)\n",
    "bin_data['RoundA_from_founded_binned'] = pd.qcut(bin_data['RoundA_from_founded'], q=NUM_LABEL,precision = 0)\n",
    "bin_data['RoundB_from_founded_binned'] = pd.qcut(bin_data['RoundB_from_founded'], q=NUM_LABEL,precision = 0)\n",
    "bin_data['RoundC_from_founded_binned'] = pd.qcut(bin_data['RoundC_from_founded'], q=NUM_LABEL,precision = 0)\n",
    "bin_data['RoundD_from_founded_binned'] = pd.qcut(bin_data['RoundD_from_founded'], q=NUM_LABEL,precision = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data['PreSeries_from_founded_binned'] = lb_make.fit_transform(bin_data['PreSeries_from_founded_binned'])\n",
    "bin_data['RoundA_from_founded_binned'] = lb_make.fit_transform(bin_data['RoundA_from_founded_binned'])\n",
    "bin_data['RoundB_from_founded_binned'] = lb_make.fit_transform(bin_data['RoundB_from_founded_binned'])\n",
    "bin_data['RoundC_from_founded_binned'] = lb_make.fit_transform(bin_data['RoundC_from_founded_binned'])\n",
    "bin_data['RoundD_from_founded_binned'] = lb_make.fit_transform(bin_data['RoundD_from_founded_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_data.to_csv('dataset_bineed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part6 - Additional processing for front-end display : Continent names, make strings to number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Continent names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_p6 = bin_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique country codes and their corresponding frequency\n",
    "countries = dataset_p6.country_code.value_counts()\n",
    "countries = pd.DataFrame(data={'country_code':countries.index,'frequency':countries.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialze a list to store the unknown country codes/ regions that we want to put to the 'others' category\n",
    "others=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for Unknown\n",
      "failed for ROM\n",
      "failed for TAN\n",
      "failed for Unknown\n",
      "failed for ROM\n",
      "failed for TAN\n"
     ]
    }
   ],
   "source": [
    "# For each country code, get it's country object\n",
    "for index, row in countries.iterrows():\n",
    "    try:\n",
    "        countries.loc[index, 'object'] = pycountry.countries.get(alpha_3=row['country_code'])\n",
    "    except:\n",
    "        others.append(row['country_code'])\n",
    "        print('failed for', row['country_code'])\n",
    "        \n",
    "# For each country object, get it's country's continent code\n",
    "for index, row in countries.iterrows():\n",
    "    try:\n",
    "        countries.loc[index, 'continent'] = pc.country_alpha2_to_continent_code(row['object'].alpha_2)\n",
    "    except:\n",
    "        others.append(row['country_code'])\n",
    "        print('failed for', row['country_code'])\n",
    "        \n",
    "# For each country continent code, get it's country's continent name       \n",
    "for index, row in countries.iterrows():\n",
    "    try:\n",
    "        countries.loc[index, 'continent_name'] = pc.convert_continent_code_to_continent_name(row['continent'])\n",
    "    except:\n",
    "        others.append(row['country_code'])\n",
    "        print('failed for', row['country_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since number of companies in these two continents is small -> put them in Others category\n",
    "others.extend(['Oceania','Africa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the library put 'USA' in continent 'North America', but the number of companies in USA is huge \n",
    "# -> separate a category for USA, other countries from NA and SA are put into 'Americas'\n",
    "for index, row in countries.iterrows():\n",
    "    if row['country_code']=='USA':\n",
    "        countries.loc[index, 'final_continent_name'] = row['country_code']\n",
    "    elif row['country_code'] in others or row['continent_name'] in others:\n",
    "        countries.loc[index, 'final_continent_name'] = 'Others'\n",
    "    elif row['continent'] in ['NA','SA']:\n",
    "        countries.loc[index, 'final_continent_name'] = 'Americas'\n",
    "    else:\n",
    "        countries.loc[index, 'final_continent_name'] = row['continent_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_continent_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Americas</th>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asia</th>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>1640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>5448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      frequency\n",
       "final_continent_name           \n",
       "Americas                    385\n",
       "Asia                       1235\n",
       "Europe                     1640\n",
       "Others                      157\n",
       "USA                        5448"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display how many companies belongs to each category\n",
    "countries.groupby('final_continent_name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_p6 = dataset_p6.merge(countries[['country_code','final_continent_name']],on='country_code')\n",
    "dataset_p6 = dataset_p6.rename(columns={'final_continent_name':'country_continent_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove 'days' from certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These entries are in timedelta format / string format\n",
    "days = ['PreSeries_from_founded',\n",
    "        'RoundA_from_founded',\n",
    " 'RoundA_from_PreSeries',\n",
    " 'RoundB_from_RoundA',\n",
    " 'RoundB_from_PreSeries',\n",
    " 'RoundB_from_founded',\n",
    " 'RoundC_from_RoundB',\n",
    " 'RoundC_from_RoundA',\n",
    " 'RoundC_from_PreSeries',\n",
    " 'RoundC_from_founded',\n",
    " 'RoundD_from_RoundC',\n",
    " 'RoundD_from_RoundB',\n",
    " 'RoundD_from_RoundA',\n",
    " 'RoundD_from_PreSeries',\n",
    " 'RoundD_from_founded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make them into number which directly represent how many days\n",
    "for day in days:\n",
    "    dataset_p6[day] = pd.to_timedelta(dataset_p6[day]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and save the final dataset\n",
    "dataset_p6.to_csv(output_path+'dataset.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
